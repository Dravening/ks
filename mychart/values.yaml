# Default values for mychart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: registry-edge.cosmoplat.com/kubesphere/beego-web
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.1.3"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8088

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

# application-token.txt
applicationTokenTxt: |-

# basic-realm.properties
basicRealmProperties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  #The format is
  #  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]
  #
  # Supported roles are: admin, manager, creator, guest
  #
  # 'user' must always be present
  #
  # Prefix with 'group:' for group information for the user.
  #

  # BASIC authentication, password is same as user name
  admin:   MD5:21232f297a57a5a743894a0e4a801fc3,user,admin
  guest:   MD5:084e0343a0486ff05530df6c705c8bb4,user,guest
  creator: MD5:ee2433259b0fe399b40e81d2c98a38b6,user,creator
  manager: MD5:1d0258c2440a8d19e716292b231e3190,user,manager
  user1:   MD5:24c9e15e52afc47c225b757e7bee1f9d,user,manager,creator,group:dev
  user2:   MD5:7e58d63b60197ceb55a1c487989a3720,user,manager,creator,group:dev
  user3:   MD5:92877af70a45fd6a2ed7fe81e1236b78,user,manager,creator,group:test
  user4:   MD5:3f02ebe3d7929b091e3d8ccfde2f3bc6,user,manager,creator,group:test

  #
  # To compute the MD5 run the following command:
  #
  # OSX:
  #      $ echo -n "<password>" | md5
  #
  # Linux:
  #      $ echo -n "<password>" | md5sum
  #

# credential-stores.properties
credentialStoresProperties: |-
  #
  # Copyright 2020 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  # Use this file to enable the use of credential stores with Data Collector.

  # IMPORTANT: This file includes a set of properties for each credential store type.
  # Property names include the default credential store IDs: jks,aws,azure,cyberark,thycotic,vault.
  # When you use custom IDs, you must update the corresponding property names.

  # To use multiple credential stores of the same type, make sure each credential store
  # has a set of related properties defined. Make sure the property names include
  # the appropriate credential store ID.

  ################################################
  #       Data Collector Credential Stores       #
  ################################################

  # Defines the credential stores for Data Collector to use. Specify a comma-separated list
  # of unique credential store IDs.
  #credentialStores=jks,aws,azure,cyberark,thycotic,vault

  ################################################
  # Java Keystore Credential Store Configuration #
  ################################################

  # The following properties are for a Java keystore credential store that uses the 'jks'
  # default credential store ID. If you specified a custom ID in the credentialStores property
  # above, replace 'jks' in the property names with the custom ID.

  # Defines the implementation of the 'jks' credential store
  # Update 'jks' in the property name as needed, but do not change the definition of this property.
  credentialStore.jks.def=streamsets-datacollector-jks-credentialstore-lib::com_streamsets_datacollector_credential_javakeystore_JavaKeyStoreCredentialStore

  # A Java keystore credential store can be of type JCEKS or PKCS12
  credentialStore.jks.config.keystore.type=PKCS12

  # The location of the Java keystore. Specify an absolute path or a path relative to the
  # $SDC_CONF directory.
  credentialStore.jks.config.keystore.file=jks-credentialStore.pkcs12

  # The password to access the Java keystore
  credentialStore.jks.config.keystore.storePassword=changeIt

  # The minimum refresh millis used to reload the keystore file
  #credentialStore.jks.config.keystore.file.min.refresh.millis=10000

  ############################################################
  #    AWS Secrets Manager Credential Store Configuration    #
  ############################################################

  # The following properties are for an AWS Secrets Manager credential store that uses the 'aws'
  # default credential store ID. If you specified a custom ID in the credentialStores property,
  # replace 'aws' in the property names with the custom ID.

  # Defines the implementation of the 'aws' credential store
  # Update 'aws' in the property name as needed, but do not change the definition of this property.
  credentialStore.aws.def=streamsets-datacollector-aws-secrets-manager-credentialstore-lib::com_streamsets_datacollector_credential_aws_secrets_manager_AWSSecretsManagerCredentialStore

  # Default name-key separator for the name parameter in credential functions
  credentialStore.aws.config.nameKey.separator=&

  # AWS Region
  credentialStore.aws.config.region=<MUST BE SET>

  # It must be: accessKeys or instanceProfile
  credentialStore.aws.config.security.method=accessKeys

  # AWS access key
  credentialStore.aws.config.access.key=<MUST BE SET IF ACCESS KEYS IS USED AS A SECURITY METHOD>

  # AWS secret key
  credentialStore.aws.config.secret.key=<MUST BE SET IF ACCESS KEYS IS USED AS A SECURITY METHOD>

  # Secrets cache max size
  # Maximum number of secrets to cache locally
  credentialStore.aws.config.cache.max.size=1024

  # Secrets cache TTL
  # The number of milliseconds that a cached secret is considered valid before requiring a refresh
  # The default is equivalent to 1 hour
  credentialStore.aws.config.cache.ttl.millis=3600000

  # Requires a group secret for each secret
  credentialStore.aws.config.enforceEntryGroup=false

  ########################################################
  #    Azure Key Vault Credential Store Configuration    #
  ########################################################

  # The following properties are for an Azure Key Vault credential store that uses the 'azure'
  # default credential store ID. If you specified a custom ID in the credentialStores property,
  # replace 'azure' in the property names with the custom ID.

  # Defines the implementation of the 'azure' credential store
  # Update 'azure' in the property name as needed, but do not change the definition of this property.
  credentialStore.azure.def=streamsets-datacollector-azure-keyvault-credentialstore-lib::com_streamsets_datacollector_credential_azure_keyvault_AzureKeyVaultCredentialStore

  # Credential refresh interval
  # How long a credential can be cached locally before fetching it again from Azure Key Vault.
  credentialStore.azure.config.credential.refresh.millis=30000

  # Credential retry interval
  # How long to wait before retrying to fetch a credential from Azure Key Vault in case of errors.
  # This retry delay is not blocking. Locally, it will fail immediately.
  credentialStore.azure.config.credential.retry.millis=15000

  # Azure Key Vault credential provider URL
  # This property must be set.
  # credentialStore.azure.config.vault.url=https://<YOUR_KEY_VAULT>.vault.azure.net/

  # Azure Key Vault client ID for this Data Collector
  #credentialStore.azure.config.client.id=<MUST BE SET>

  # Azure Key Vault client key for this Data Collector
  #credentialStore.azure.config.client.key=<MUST BE SET>

  # ERequires a group secret for each secret
  credentialStore.azure.config.enforceEntryGroup=false

  #################################################
  #    CyberArk Credential Store Configuration    #
  #################################################

  # The following properties are for a CyberArk credential store that uses the 'cyberark'
  # default credential store ID. If you specified a custom ID in the credentialStores property,
  # replace 'cyberark' in the property names with the custom ID.

  # Defines the implementation of the 'cyberark' credential store
  # Update 'cyberark' in the property name as needed, but do not change the definition of this property.
  credentialStore.cyberark.def=streamsets-datacollector-cyberark-credentialstore-lib::com_streamsets_datacollector_credential_cyberark_CyberArkCredentialStore

  # Credential refresh interval
  # How long a credential can be cached locally before fetching it again from CyberArk.
  #credentialStore.cyberark.config.credential.refresh.millis=30000

  # Credential retry interval
  # How long to wait before retrying to fetch a credential from CyberArk in case of errors.
  # This retry delay is not blocking. Locally, it will fail immediately.
  #credentialStore.cyberark.config.credential.retry.millis=15000

  # Connector type to CyberArk
  # Currently 'webservices' is the only supported connector
  #credentialStore.cyberark.config.connector=webservices

  ##############################################################
  #     CyberArk Credential Store Web Service Configuration    #
  ##############################################################

  # CyberArk Central Credential Provider credential retrieval web service URL
  credentialStore.cyberark.config.ws.url=https://<HOST>:<PORT>/AIMWebService/api/Accounts

  # CyberArk application ID for this Data Collector
  credentialStore.cyberark.config.ws.appId=<MUST BE SET>

  # Maximum number of concurrent web service calls to CyberArk
  #credentialStore.cyberark.config.ws.maxConcurrentConnections=10

  # HTTP connection inactivity check
  #credentialStore.cyberark.config.ws.validateAfterInactivity.millis=60000

  # TCP and HTTP connection timeout
  #credentialStore.cyberark.config.ws.connectionTimeout.millis=10000

  # Default separator for CyberArk safe, folder, object name, and object element used in the
  # name parameter in credential functions.
  #credentialStore.cyberark.config.ws.nameSeparator=&

  # HTTP authentication mechanism used by CyberArk Central Credential Provider web services
  # Possible values: none, basic, digest
  #credentialStore.cyberark.config.ws.http.authentication=none

  # User name when using basic or digest authentication
  #credentialStore.cyberark.config.ws.http.authentication.user=

  # Password when using basic or digest authentication
  #credentialStore.cyberark.config.ws.http.authentication.password=

  # When using HTTPS and the server certificate is not signed by a public CA, a truststore
  # with the public certificate must be available in this truststore file, or in the JDK default truststore.
  # Specify an absolute path or a path relative to the $SDC_CONF directory.
  #credentialStore.cyberark.config.ws.truststoreFile=

  # The password to access the truststore file
  #credentialStore.cyberark.config.ws.truststorePassword=

  # HTTPS supported protocols
  #credentialStore.cyberark.config.ws.supportedProtocols=TLSv1.2

  # Determines if the hostname of the CyberArk Central Credential Provider web service should be
  # verified against the domain defined in the HTTPS certificate.
  #credentialStore.cyberark.config.ws.hostnameVerifier.skip=false

  # When using HTTPS and the CyberArk Central Credential Provider web service is configured to require client side
  # certificates, the client certificate must be available in this keystore file, or in the JDK default truststore.
  # Specify an absolute path or a path relative to the $SDC_CONF directory.
  #credentialStore.cyberark.config.ws.keystoreFile=

  # The password to access the keystore file
  #credentialStore.cyberark.config.ws.keystorePassword=

  # The password to access the certificate within the keystore file
  #credentialStore.cyberark.config.ws.keyPassword=

  # Requires a group secret for each secret
  credentialStore.cyberark.config.enforceEntryGroup=false

  ########################################################
  #    Hashicorp Vault Credential Store Configuration    #
  ########################################################

  # The following properties are for a Hashicorp Vault credential store that uses the 'vault'
  # default credential store ID. If you specified a custom ID in the credentialStores property,
  # replace 'vault' in the property names with the custom ID.

  # Defines the implementation of the 'vault' credential store
  # Update 'vault' in the property name as needed, but do not change the definition of this property.
  credentialStore.vault.def=streamsets-datacollector-vault-credentialstore-lib::com_streamsets_datacollector_credential_vault_VaultCredentialStore

  # Default path-key separator for the name parameter in credential functions
  credentialStore.vault.config.pathKey.separator=&

  # URL of the Vault server to connect to
  credentialStore.vault.config.addr=http://localhost:8200

  # AppRole mode (recommended)
  credentialStore.vault.config.role.id=
  credentialStore.vault.config.secret.id=${file("vault-secret-id")}

  #
  # The Vault User ID is generated by hashing the MAC address belonging to the network interface assigned
  # the IP address of hostname -f. It can also be retrieved by the show-vault-id command of the
  # StreamSets executable.
  #

  # Data Collector authenticates with Vault using the AppId authentication backend. The app-id must be specified below.
  # credentialStore.vault.config.app.id=

  # Optional Settings

  # Supported KV Secret Engine version 1 by default. Possible values: 1 or 2.
  credentialStore.vault.config.version=1

  # Define namespaces for Vault Enterprise
  #credentialStore.vault.config.namespace=

  # The renewal interval must be shorter than the shortest lease issued by Vault including auth tokens.
  credentialStore.vault.config.lease.renewal.interval.sec=60
  credentialStore.vault.config.lease.expiration.buffer.sec=120
  credentialStore.vault.config.open.timeout=0
  credentialStore.vault.config.proxy.address=
  credentialStore.vault.config.proxy.port=8080
  credentialStore.vault.config.proxy.username=
  credentialStore.vault.config.proxy.password=
  credentialStore.vault.config.read.timeout=0
  credentialStore.vault.config.ssl.enabled.protocols=TLSv1.2,TLSv1.3
  credentialStore.vault.config.ssl.truststore.file=
  credentialStore.vault.config.ssl.truststore.password=
  credentialStore.vault.config.ssl.verify=true
  credentialStore.vault.config.ssl.timeout=0
  credentialStore.vault.config.timeout=0

  # Requires a group secret for each secret
  credentialStore.vault.config.enforceEntryGroup=false

  #####################################################################
  #    Thycotic Secret Server Credential Store Configuration          #
  #####################################################################

  # The following properties are for an Thycotic Secret Server credential store that uses the 'thycotic'
  # default credential store ID. If you specified a custom ID in the credentialStores property,
  # replace 'thycotic' in the property names with the custom ID.

  # Defines the implementation of the 'thycotic' credential store.
  # Update 'thycotic' in the property name as needed, but do not change the definition of this property.
  credentialStore.thycotic.def=streamsets-datacollector-thycotic-credentialstore-lib::com_streamsets_datacollector_credential_thycotic_ThycoticCredentialStore

  # Thycotic Secret Server URL. Use the following format: https://<host name>:<port number>.
  # Use HTTPS to avoid unencrypted communication.
  credentialStore.thycotic.config.url=<MUST BE SET>

  # User name to connect to Thycotic Secret Server
  credentialStore.thycotic.config.username=<MUST BE SET>

  # Password to connect to Thycotic Secret Server
  credentialStore.thycotic.config.password=${file("thycotic-secret-password")}

  # Cache expiration time
  credentialStore.thycotic.config.credential.cache.inactivityExpiration.seconds=1800

  # Credential refresh interval
  # How long a credential can be cached locally before fetching it again from Thycotic Secret Server.
  credentialStore.thycotic.config.credential.refresh.seconds=30000

  # Credential retry interval
  # How long to wait before retrying to fetch a credential from Thycotic Secret Server in the case of an error.
  credentialStore.thycotic.config.credential.retry.seconds=15000

  # Buffer for expiring auth tokens. Data Collector renews tokens that expire in less than
  # the specified number of seconds. Default is 1201.
  credentialStore.thycotic.config.token.expiration.buffer.seconds=1201

  # SSL/TLS-enabled protocols. Versions TLSv1.2 or later are recommended. Default is TLSv1.2,TLSv1.3
  credentialStore.thycotic.config.ssl.enabled.protocols=TLSv1.2,TLSv1.3

  # Path to a Java truststore file. Required when using a private CA or certificates not trusted
  # by the Java default truststore.
  credentialStore.thycotic.config.ssl.truststore.file=

  # Password for the truststore file
  credentialStore.thycotic.config.ssl.truststore.password=

  # Whether to verify that the Thycotic server hostname matches its certificate.
  # Default is true. False is not recommended.
  credentialStore.thycotic.config.ssl.verify=true

  # Timeout for the SSL/TLS handshake in milliseconds. Default is 0 for no limit.
  credentialStore.thycotic.config.ssl.timeout=0

  # Separator to use for the Thycotic Secret Server secret ID and field name values in the
  # credential name argument used in credential functions.
  credentialStore.thycotic.config.nameSeparator=-

  # Milliseconds to wait for data before timing out.
  # Default is 0 for no limit.
  credentialStore.thycotic.config.read.timeout=0

  # Timeout to establish an HTTP connection to Thycotic Secret Server in milliseconds.
  # Default is 0 for no limit.
  credentialStore.thycotic.config.open.timeout=0

  # Timeout in milliseconds to read from Thycotic Secret Server after a connection has been established.
  # Default is 0 for no limit.
  credentialStore.thycotic.config.timeout=0

  # Requires a group secret for each secret
  credentialStore.thycotic.config.enforceEntryGroup=false

# database-realm.properties
databaseRealmProperties: |-
  jdbcdriver=com.mysql.jdbc.Driver
  url=jdbc:mysql://10.206.68.1:30234/datathread
  username=admin
  password=FviqAJDYbu
  usertable=users
  usertablekey=id
  usertableuserfield=username
  usertablepasswordfield=password
  roletable=roles
  roletablekey=id
  roletablerolefield=rolename
  userroletable=user_role
  userroletableuserkey=user_id
  userroletablerolekey=role_id
  remoteurl=http://dt-co-server-test.hdt.cosmoplat.com
  loginflag=remote

# digest-realm.properties
digestRealmProperties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  #The format is
  #  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]
  #
  # Supported roles are: admin, manager, creator, guest
  #
  # 'user' must always be present
  #
  # Prefix with 'group:' for group information for the user.
  #

  # DIGEST authentication, password is same as user name
  admin:   MD5:184b0de86a7c6e86924b5978c97d6969,user,admin
  guest:   MD5:bb1b090606eb8f94cd0f03b3f37f1cf0,user,guest
  creator: MD5:e71e2f1c732f81a723285369a4e5ca89,user,creator
  manager: MD5:97500f290705dbbe5be2889bcbdd9ed0,user,manager
  user1:   MD5:d06240e76c12549a2efe9262f2ef2cd8,user,manager,creator,group:dev
  user2:   MD5:791644a3c770ad2cf72c6c6947f8c1d6,user,manager,creator,group:dev
  user3:   MD5:8a1f9771c26853e0bbeb178519d9d4a9,user,manager,creator,group:test
  user4:   MD5:bd63a028da4987d08da7ff5a6026888c,user,manager,creator,group:test

  #
  # To compute the MD5 run the following command:
  #
  # OSX:
  #
  #      $ echo -n "<username>:<realm>:<password>" | md5
  #
  # Linux:
  #
  #      $ echo -n "<username>:<realm>:<password>" | md5sum
  #

# dpm-url.txt
dpmUrlTxt: |-
  http://localhost:18631

# dpm.properties
dpmProperties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  #
  # Control Hub Enabled
  # If true HTTP Authentication for Data Collector will be configured to use Control Hub SSO Authentication.
  #
  dpm.enabled=false

  #
  # Base URL of the Remote Service
  # In a real deployment the security service must be encrypted (HTTPS)
  #
  dpm.base.url=@dpm-url.txt@

  #
  # Registration attempts.
  # There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.
  #
  dpm.registration.retry.attempts=5

  #
  # Frequency of validation of user and app authentication tokens.
  # As part of this validation all information about the principal is refreshed.
  #
  dpm.security.validationTokenFrequency.secs=60

  #
  # Application Token
  #
  dpm.appAuthToken=@application-token.txt@

  #
  # Labels for this Data Collector to report the Control Hub
  #
  dpm.remote.control.job.labels=all

  #
  # Data Collector Ping Frequency to Control Hub (in milliseconds)
  #
  dpm.remote.control.ping.frequency=5000

  #
  # App to send remote control events
  #
  dpm.remote.control.events.recipient=jobrunner-app

  #
  # Apps to send Data Collector Process metrics (CPU Load and Heap Memory Usage)
  #
  dpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app

  #
  # Frequency to send pipeline status events (all remote pipelines and local running pipelines) and
  # Data Collector process metrics like CPU load and heap memory usage
  #
  dpm.remote.control.status.events.interval = 60000


  dpm.remote.deployment.id=

  #
  # Indicates if the redirection to Control Hub SSO is done using HTML META refresh.
  # This is useful for environment that rewrite redirect headers.
  #
  http.meta.redirect.to.sso=false


  # Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting
  # only takes effect when sdc is control hub enabled and stage impersonation is set to true
  #
  #dpm.alias.name.enabled=true

# email-password.txt
emailPasswordTxt: |-
  password

# form-realm.properties
formRealmProperties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  #The format is
  #  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]
  #
  # Supported roles are: admin, manager, creator, guest
  #
  # 'user' must always be present
  #
  # Prefix with 'group:' for group information for the user.
  #

  # FORM authentication, password is same as user name
  admin:   MD5:21232f297a57a5a743894a0e4a801fc3,user,admin
  guest:   MD5:084e0343a0486ff05530df6c705c8bb4,user,guest
  creator: MD5:ee2433259b0fe399b40e81d2c98a38b6,user,creator
  manager: MD5:1d0258c2440a8d19e716292b231e3190,user,manager
  user1:   MD5:24c9e15e52afc47c225b757e7bee1f9d,user,manager,creator,group:dev
  user2:   MD5:7e58d63b60197ceb55a1c487989a3720,user,manager,creator,group:dev
  user3:   MD5:92877af70a45fd6a2ed7fe81e1236b78,user,manager,creator,group:test
  user4:   MD5:3f02ebe3d7929b091e3d8ccfde2f3bc6,user,manager,creator,group:test


  #
  # To compute the MD5 run the following command:
  #
  # OSX:
  #      $ echo -n "<password>" | md5
  #
  # Linux:
  #      $ echo -n "<password>" | md5sum
  #

# jdbc.properties
jdbcProperties: |-
  jdbc.driver=com.mysql.cj.jdbc.Driver
  jdbc.url=jdbc:mysql://10.206.68.1:30234/datathread?useSSL=false&autoReconnect=true&serverTimezone=GMT%2B8&rewriteBatchedStatements=true
  jdbc.username=admin
  jdbc.password=FviqAJDYbu

# keystore-password.txt
keystorePasswordTxt: |-
  password

# keystore.jks
keystore.jks: |-

# ldap-bind-password.txt
ldap-bind-password.txt: |-
  password

# ldap-login.conf
ldap-login.conf: |-
  ldap {
    com.streamsets.datacollector.http.LdapLoginModule required
    debug="false"
    useLdaps="false"
    useStartTLS="false"
    contextFactory="com.sun.jndi.ldap.LdapCtxFactory"
    hostname="#### LDAP HOST ####"
    port="389"
    bindDn="cn=Directory Manager"
    bindPassword="@ldap-bind-password.txt@"
    forceBindingLogin="false"
    userBaseDn="ou=people,dc=alcatel"
    userIdAttribute="uid"
    userPasswordAttribute="userPassword"
    userObjectClass="inetOrgPerson"
    userFilter="uid={user}"
    roleBaseDn="ou=groups,dc=example,dc=com"
    roleNameAttribute="cn"
    roleMemberAttribute="member"
    roleObjectClass="groupOfNames"
    roleFilter="member={dn}";
  };

# remote-realm.properties
remote-realm.properties: |-
  remote.url=http://dt-co-server-test.hdt.cosmoplat.com
  login.flag=remote
  remote.user.create.interface=/center/sys-user/register
  remote.user.get.info.interface=/center/sys-user/getUserInfo
  remote.user.login.interface=/center/login
  remote.user.reset.password.interface=/center/sys-user/passwordReset
  remote.user.change.password.interface=/center/sys-user/operatePassword
  remote.user.update.interface=/center/sys-user/operateUser
  remote.user.list.query.interface=/center/sys-user/getUserList
  remote.user.group.list.query.interface=/center/sys-company/checkCompany

# sdc-log4j.properties
sdc-log4j.properties: |-
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.

  # /dev/null appender
  log4j.appender.null=org.apache.log4j.FileAppender
  log4j.appender.null.File=/dev/null
  log4j.appender.null.layout=org.apache.log4j.PatternLayout
  log4j.appender.null.layout.ConversionPattern=null

  # <stdout> appender
  log4j.appender.stdout=org.apache.log4j.ConsoleAppender
  log4j.appender.stdout.Target=System.out
  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
  log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [runner:%X{s-runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n

  # sdc.log appender
  log4j.appender.streamsets=org.apache.log4j.RollingFileAppender
  log4j.appender.streamsets.MaxFileSize=256MB
  log4j.appender.streamsets.MaxBackupIndex=10
  log4j.appender.streamsets.File=${sdc.log.dir}/sdc.log
  log4j.appender.streamsets.Append=true
  log4j.appender.streamsets.layout=org.apache.log4j.PatternLayout
  log4j.appender.streamsets.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [runner:%X{s-runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n
  # log4j.appender.file3=org.apache.log4j.DailyRollingFileAppender
  # log4j.appender.file3.Append=true
  # log4j.appender.file3.DatePattern='_'yyyy-MM-dd
  # log4j.appender.file3.File=/opt/data_process/logs/edc_report.log
  # log4j.appender.file3.Threshold=INFO
  # log4j.appender.file3.Encoding=UTF-8
  # log4j.appender.file3.layout=org.apache.log4j.PatternLayout
  # log4j.appender.file3.layout.ConversionPattern=%p %d{yyyy-MM-dd HH\:mm\:ss} %C.%M(%L) | %m %n

  log4j.rootLogger=INFO, streamsets
  log4j.logger.com.streamsets=INFO
  log4j.logger.org.eclipse.jetty=WARN
  log4j.logger.com.amazonaws.services.kinesis.clientlibrary.lib.worker.SequenceNumberValidator=WARN
  log4j.logger.org.apache.kafka=WARN
  # log4j.logger.com.datastax.driver.core.QueryLogger.SLOW=DEBUG
  log4j.logger.com.datastax.driver.core.QueryLogger.SLOW=INFO

# sdc-security.policy
sdc-security.policy: |-
  // Copyright 2017 StreamSets Inc.
  //
  // Licensed under the Apache License, Version 2.0 (the "License");
  // you may not use this file except in compliance with the License.
  // You may obtain a copy of the License at
  //
  //     http://www.apache.org/licenses/LICENSE-2.0
  //
  // Unless required by applicable law or agreed to in writing, software
  // distributed under the License is distributed on an "AS IS" BASIS,
  // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  // See the License for the specific language governing permissions and
  // limitations under the License.

  // StreamSets Data Collector Policy File

  // StreamSets code base:
  grant codebase "file://${sdc.bootstrapLib.dir}/-" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.rootLib.dir}/*" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.apiLib.dir}/*" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.asterClientLib.dir}/*" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.containerLib.dir}/*" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.librariesExtras.dir}/-" {
    permission java.security.AllPermission;
  };
  // StreamSets stage libraries code base:
  grant codebase "file://${sdc.libraries.dir}/-" {
    permission java.security.AllPermission;
  };
  grant codebase "file://${sdc.userLibs.dir}/streamsets-datacollector-dev-lib/-" {
    permission java.security.AllPermission;
  };
  // Groovy will parse files in a different context, so we need to grant it additional privileges
  grant codebase "file:/groovy/script" {
    permission java.lang.RuntimePermission "getClassLoader";
  };

  // For details on how to grant specific permissions, refer to the Java Permissions Documentation:
  //   http://docs.oracle.com/javase/7/docs/technotes/guides/security/permissions.html

  // User stage libraries code base:
  grant codebase "file://${sdc.userLibs.dir}/-" {
    permission java.util.PropertyPermission "*", "read";
    permission java.lang.RuntimePermission "accessDeclaredMembers";
    permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
    permission java.io.FilePermission "${sdc.dist.dir}/user-libs/streamsets-datacollector-dev-lib/lib/*", "read";
  };

  // For JARs to be available to all stage libraries
  grant codebase "file://${sdc.libsCommon.dir}/-" {
    permission java.security.AllPermission;
  };

# sdc.properties
sdc.properties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  # HTTP configuration

  # The base URL of Data Collector, used to create email alert messages.
  # If not set http://<hostname>:<http.port> is used
  # <hostname> is either taken from http.bindHost or resolved using
  # 'hostname -f' if not configured.
  #sdc.base.http.url=http://<hostname>:<port>

  # Hostname or IP address that Data Collector will bind to.
  # Default is 0.0.0.0 that will bind to all interfaces.
  #http.bindHost=0.0.0.0

  # Maximum number of HTTP servicing threads.
  #http.maxThreads=200

  # The port Data Collector uses as an HTTP endpoint.
  # If different from -1, the Data Collector will run on this port
  # If 0, the Data Collector will pick up a random port
  # If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint
  # will redirect to the HTTPS endpoint.
  http.port=18630

  # HTTPS configuration

  # TThe port Data Collector uses as an HTTPS endpoint.
  # If different from -1, the Data Collector will run over SSL on this port
  # If 0, the Data Collector will use a random port
  https.port=-1

  # Enables HTTP/2 support for the Data Collector UI/REST API. If you are using any clients
  # that do not support ALPN for protocol negotiation, leave this option disabled.
  http2.enable=false

  # Reverse Proxy / Load Balancer configuration

  # Data Collector will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port
  # headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.
  # Set to true when hosting Data Collector behind a reverse proxy / load balancer.
  http.enable.forwarded.requests=false

  # Java keystore file, in the Data Collector 'etc/' configuration directory
  https.keystore.path=keystore.jks

  # Password for the keystore file,
  # By default, the password is loaded from 'keystore-password.txt'
  # in the Data Collector 'etc/' configuration directory
  https.keystore.password=${file("keystore-password.txt")}

  # Path to keystore file on the worker node. This should always be an absolute location
  https.cluster.keystore.path=/opt/security/jks/sdc-keystore.jks

  # Password for keystore file on the worker node
  https.cluster.keystore.password=${file("/opt/security/jks/keystore-password.txt")}

  # Truststore configs
  # By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore

  # Java truststore file on the gateway Data Collector which stores certificates to trust identity of workers
  #https.truststore.path=

  # Password for truststore file
  #https.truststore.password=

  # Path to truststore file on worker node. This should always be an absolute location
  #https.cluster.truststore.path=/opt/security/jks/sdc-truststore.jks

  # Password for truststore file on worker
  #https.cluster.truststore.password=${file("/opt/security/jks/truststore-password.txt")}

  # HTTP Session Timeout
  # Max period of inactivity, after which the HTTP session is invalidated, in seconds.
  # Default value is 86400 seconds (24 hours)
  # value -1 means no timeout
  http.session.max.inactive.interval=86400

  # The authentication for the HTTP endpoint of Data Collector
  # Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'
  #
  http.authentication=form

  # Authentication Login Module
  # Valid values are: 'file' and 'ldap'
  # For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,
  #   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).
  # For 'ldap', the authentication and role information is read from a LDAP server
  # and LDAP connection information is read from etc/ldap-login.conf.
  # http.authentication.login.module=file
  http.authentication.login.module=remote

  # The realm used for authentication
  # A file with the realm name and '.properties' extension must exist in the Data Collector configuration directory
  # If this property is not set, the realm name is '<http.authentication>-realm'
  #http.digest.realm=local-realm

  # Check the permissions of the realm file should be owner only
  http.realm.file.permission.check=false

  # LDAP group to Data Collector role mapping
  # the mapping is specified as the following pattern:
  #    <ldap-group>:<sdc-role>(,<sdc-role>)*(;<ldap-group>:<sdc-role>(,<sdc-role>)*)*
  # e.g. Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;
  http.authentication.ldap.role.mapping=

  # LDAP login module name as present in the JAAS config file.
  # If no value is specified, the login module name is assumed to be "ldap"
  ldap.login.module.name=ldap

  # HTTP access control (CORS)
  http.access.control.allow.origin=*
  http.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call
  http.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET
  http.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD

  # Runs the Data Collector within a Kerberos session which is propagated to all stages.
  # This is useful for stages that require Kerberos authentication with the services they interact with
  kerberos.client.enabled=false

  # The Kerberos principal to use for the Kerberos session.
  # It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',
  # the hostname will be replaced with the actual complete hostname of Data Collector as advertised by the
  # unix command 'hostname -f'
  kerberos.client.principal=sdc/_HOST@EXAMPLE.COM

  # The location of the keytab file for the specified principal. If the path is relative, the keytab file will be
  # looked for in the Data Collector configuration directory
  kerberos.client.keytab=sdc.keytab

  # Maximal batch size for preview
  preview.maxBatchSize=1000
  # Maximal number of batches for preview
  preview.maxBatches=10
  # Maximal batch size for pipeline run
  production.maxBatchSize=50000

  #Specifies the buffer size for Overrun parsers - including JSON, XML and CSV.
  #This parameter is specified in bytes, and must be greater than
  #1048576 bytes (which is the default size).
  #parser.limit=5335040

  #This option determines the number of error records, per stage, that will be retained in memory when the pipeline is
  #running. If set to zero, error records will not be retained in memory.
  #If the specified limit is reached the oldest records will be discarded to make room for the newest one.
  production.maxErrorRecordsPerStage=100

  #This option determines the number of pipeline errors that will be retained in memory when the pipeline is
  #running. If set to zero, pipeline errors will not be retained in memory.
  #If the specified limit is reached the oldest error will be discarded to make room for the newest one.
  production.maxPipelineErrors=100

  # Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint
  max.logtail.concurrent.requests=5

  # Max number of concurrent WebSocket calls allowed
  max.webSockets.concurrent.requests=15

  # Pipeline Sharing / ACLs
  pipeline.access.control.enabled=false

  # Customize header title for the Data Collector UI
  # You can pass any HTML tags here
  # Example:
  #   For Text  -  <span class="navbar-brand">New Brand Name</span>
  #   For Image -  <img src="assets/add.png">
  ui.header.title=

  ui.local.help.base.url=/docs
  ui.hosted.help.base.url=https://www.streamsets.com/documentation/datacollector/3.22.3-SNAPSHOT/userguide/help
  # ui.registration.url=https://registration.streamsets.com/register
  # ui.account.registration.url=

  ui.refresh.interval.ms=2000
  ui.jvmMetrics.refresh.interval.ms=4000

  # If set to true, the Data Collector UI will use WebSocket to fetch pipeline status/metrics/alerts. Otherwise, the UI
  # will poll every few seconds to get the pipeline status/metrics/alerts.
  ui.enable.webSocket=true

  # Number of changes supported by undo/redo functionality.
  # UI archives pipeline configuration/rules in browser memory to support undo/redo functionality.
  ui.undo.limit=10

  # Default mode for stage configuration view unless specifically selected by
  # the user on the canvas for the pipeline stage.
  # Set to ADVANCED to show advanced configurations by default
  # ui.default.configuration.view=BASIC

  # SMTP configuration to send alert emails
  # All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'
  mail.transport.protocol=smtp
  mail.smtp.host=localhost
  mail.smtp.port=25
  mail.smtp.auth=false
  mail.smtp.starttls.enable=false
  mail.smtps.host=localhost
  mail.smtps.port=465
  mail.smtps.auth=false
  # If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,
  # ${file("email-password.txt")} will load the value from the 'email-password.txt' file in the config directory (where this file is)
  xmail.username=foo
  xmail.password=${file("email-password.txt")}
  # FROM email address to use for the messages
  xmail.from.address=sdc@localhost

  #Indicates the location where runtime configuration properties can be found.
  #Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with
  #'runtime.conf_'.
  #A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration
  #properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.
  runtime.conf.location=embedded

  # Java Security properties
  #
  # Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security
  # as part of Data Collector bootstrap process. This will change JVM configuration and should not be used when embedding and running
  # multiple Data Collector instances inside the same JVM.
  #
  # We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.
  java.security.networkaddress.cache.ttl=0

  # Security Manager
  #
  # By default, when Security Manager is enabled, Data Collector will use Java security manager that only follows the specified policy.

  # Enable the Data Collector Security Manager to prevent access to Data Collector internal directories to all stages (e.g. data dir,
  # ...). Please note that there are certain JVM bugs that this manager might hit, especially on some older JVM versions.
  #security_manager.sdc_manager.enable=true

  # When Security Manager is enabled Data Collector will by default prohibit access to its internal directories regardless of what
  # the security policy specifies. The following properties allow specific access to given files inside protected directories.
  # General exceptions - use with caution, all stage libraries will be able to access those files.
  # * Access to ldap-login.conf is sadly required by the Hadoop UserGroupInfo class
  security_manager.sdc_dirs.exceptions=$SDC_CONF/ldap-login.conf

  # Exceptions for specific stage libraries
  # * Our documentation recommends default name for credential store inside ETC directory
  security_manager.sdc_dirs.exceptions.lib.streamsets-datacollector-jks-credentialstore-lib=$SDC_CONF/jks-credentialStore.pkcs12


  # Stage-specific configurations
  #
  # The following config properties are for particular stages, please refer to their documentation for further details.
  #
  # Hadoop components
  # Uncomment to enforce Hadoop components in Data Collector to always impersonate current user rather then use
  # the impersonation configuration option. Current user is a user who either started the pipeline or run preview.
  #stage.conf_hadoop.always.impersonate.current.user=true
  # Uncomment to enforce impersonated user name to be lower cased.
  #stage.conf_hadoop.always.lowercase.user=true
  #

  # Impersonate current user to connect to Hive
  stage.conf_com.streamsets.pipeline.stage.hive.impersonate.current.user=false

  # JDBC components
  # Drivers that should always be auto-loaded even if they are not JDBC 4 compliant or fails to load (comma separated list)
  #stage.conf_com.streamsets.pipeline.stage.jdbc.drivers.load=mysql.jdbc.Driver

  # Kafka stages
  # Controls where Kerberos authentication keytabs entered in stage properties are stored
  # stage.conf_kafka.keytab.location=/tmp/sdc

  # Use new version of addRecordsToQueue() in Oracle CDC origin
  stage.conf_com.streamsets.pipeline.stage.origin.jdbc.cdc.oracle.addrecordstoqueue=true

  # Shell executor
  # Controls impersonation mode
  #stage.conf_com.streamsets.pipeline.stage.executor.shell.impersonation_mode=CURRENT_USER
  # Relative or absolute path to shell that should be used to execute the shell script
  #stage.conf_com.streamsets.pipeline.stage.executor.shell.shell=sh
  # Relative or absolute path to sudo command
  #stage.conf_com.streamsets.pipeline.stage.executor.shell.sudo=sudo

  # Antenna Doctor

  # Antenna Doctor is a rule-based engine designed to help end-user self-diagnose most common issues and suggest
  # potential fixes and workarounds.

  # Uncomment to disable Antenna Doctor completely
  #antennadoctor.enable=false

  # Uncomment to disable periodical updates of the knowledge base from internet
  #antennadoctor.update.enable=false

  #Observer related

  #The size of the queueName where the pipeline queues up data rule evaluation requests.
  #Each request is for a stream and contains sampled records for all rules that apply to that lane.
  observer.queue.size=100

  #Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is
  #once cache per data rule
  observer.sampled.records.cache.size=100

  #The time to wait before dropping a data rule evaluation request if the observer queueName is full.
  observer.queue.offer.max.wait.time.ms=1000


  #Maximum number of private classloaders to allow in Data Collector.
  #Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders
  max.stage.private.classloaders=50

  # Pipeline runner pool
  # Default value is sufficient to run 22 pipelines. One pipeline requires 5 threads and pipelines share
  # threads using thread pool. Approximate runner thread pool size = (Number of Running Pipelines) * 2.2.
  # Increasing this value will not increase parallelisation of individual pipelines.
  runner.thread.pool.size=50

  # Uncomment to disable starting all previously running pipelines upon Data Collector start up
  #runner.boot.pipeline.restart=false

  # Maximal number of runners (multithreaded pipelines)
  #
  # Maximal number of source-less pipeline instances (=runners) that are allowed for a single multi-threaded
  # pipeline. The default is 50.
  pipeline.max.runners.count=50

  # Uncomment to specify a custom location for Package Manager repositories.
  # Enter a url or comma-separated list of urls.
  # Official Data Collector releases use the following repositories by default:
  # http://archives.streamsets.com/datacollector/<version>/tarball/,
  # http://archives.streamsets.com/datacollector/<version>/tarball/enterprise/ and
  # http://archives.streamsets.com/datacollector/<version>/legacy/
  # Data Collector source code builds (master branch) use the following repositories by default:
  # http://nightly.streamsets.com/datacollector/latest/tarball/,
  # http://nightly.streamsets.com/datacollector/latest/tarball/enterprise/ and
  # http://nightly.streamsets.com/datacollector/latest/legacy/ and
  #package.manager.repository.links=

  # Support bundles
  #
  # Uncomment if you need to disable the facility for automatic support bundle upload.
  #bundle.upload.enabled=false
  #
  # Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle
  # can be time consuming task (depending on size and internet speed) and pipelines can appear "frozen" during
  # the upload especially when many pipelines are failing at the same time.
  #bundle.upload.on_error=true

  # Library aliases mapping to keep backward compatibility on pipelines when library names change
  # The current aliasing mapping is to handle 1.0.0beta2 to 1.0.0 library names changes
  #
  # IMPORTANT: Under normal circumstances all these properties should not be changed
  #
  library.alias.streamsets-datacollector-apache-kafka_0_8_1_1-lib=streamsets-datacollector-apache-kafka_0_8_1-lib
  library.alias.streamsets-datacollector-apache-kafka_0_8_2_0-lib=streamsets-datacollector-apache-kafka_0_8_2-lib
  library.alias.streamsets-datacollector-apache-kafka_0_8_2_1-lib=streamsets-datacollector-apache-kafka_0_8_2-lib
  library.alias.streamsets-datacollector-cassandra_2_1_5-lib=streamsets-datacollector-cassandra_2-lib
  library.alias.streamsets-datacollector-cdh5_2_1-lib=streamsets-datacollector-cdh_5_2-lib
  library.alias.streamsets-datacollector-cdh5_2_3-lib=streamsets-datacollector-cdh_5_2-lib
  library.alias.streamsets-datacollector-cdh5_2_4-lib=streamsets-datacollector-cdh_5_2-lib
  library.alias.streamsets-datacollector-cdh5_3_0-lib=streamsets-datacollector-cdh_5_3-lib
  library.alias.streamsets-datacollector-cdh5_3_1-lib=streamsets-datacollector-cdh_5_3-lib
  library.alias.streamsets-datacollector-cdh5_3_2-lib=streamsets-datacollector-cdh_5_3-lib
  library.alias.streamsets-datacollector-cdh5_4_0-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib
  library.alias.streamsets-datacollector-cdh5_4_0-lib=streamsets-datacollector-cdh_5_4-lib
  library.alias.streamsets-datacollector-cdh5_4_1-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib
  library.alias.streamsets-datacollector-cdh5_4_1-lib=streamsets-datacollector-cdh_5_4-lib
  library.alias.streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib
  library.alias.streamsets-datacollector-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_kafka_1_2-lib
  library.alias.streamsets-datacollector-elasticsearch_1_4_4-lib=streamsets-datacollector-elasticsearch_1_4-lib
  library.alias.streamsets-datacollector-elasticsearch_1_5_0-lib=streamsets-datacollector-elasticsearch_1_5-lib
  library.alias.streamsets-datacollector-hdp_2_2_0-lib=streamsets-datacollector-hdp_2_2-lib
  library.alias.streamsets-datacollector-jython_2_7_0-lib=streamsets-datacollector-jython_2_7-lib
  library.alias.streamsets-datacollector-mongodb_3_0_2-lib=streamsets-datacollector-mongodb_3-lib
  library.alias.streamsets-datacollector-cassandra_2-lib=streamsets-datacollector-cassandra_3-lib
  library.alias.streamsets-datacollector-cdh_5_9-cluster-cdh_kafka_2_0-lib=streamsets-datacollector-cdh-spark_2_1-lib
  library.alias.streamsets-datacollector-cdh_5_10-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib
  library.alias.streamsets-datacollector-cdh_5_11-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib
  library.alias.streamsets-datacollector-cdh_5_12-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib
  library.alias.streamsets-datacollector-cdh_5_13-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib
  library.alias.streamsets-datacollector-cdh_5_14-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib


  # Stage aliases for mapping to keep backward compatibility on pipelines when stages move libraries
  # The current alias mapping is to handle moving the jdbc stages to their own library
  #
  # IMPORTANT: Under normal circumstances all these properties should not be changed
  #
  stage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget
  stage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource
  stage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource=streamsets-datacollector-omniture-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource
  stage.alias.streamsets-datacollector-cdh_5_7-cluster-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget=streamsets-datacollector-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget
  stage.alias.streamsets-datacollector-cdh_5_4-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-cdh_5_5-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-cdh_5_7-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-cdh_5_8-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-cdh_5_9-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-cdh_5_10-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor
  stage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget
  stage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget
  stage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget
  stage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget
  stage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource
  stage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor
  stage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget
  stage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget
  stage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor
  stage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget
  stage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget
  stage.alias.streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseConnectorDTarget=streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseDTarget

  stages.library.libs.remove.manager.links=streamsets-datacollector-aws-lib,streamsets-datacollector-kinesis-lib,streamsets-datacollector-azure-lib,streamsets-datacollector-google-cloud-lib,streamsets-datacollector-apache-pulsar_2-lib,streamsets-datacollector-salesforce-lib,streamsets-datacollector-couchbase_5-lib,streamsets-datacollector-databricks-ml_2-lib,streamsets-datacollector-mleap-lib,streamsets-datacollector-tensorflow-lib,streamsets-datacollector-bigtable-lib
  stages.library.stage.remove.manager.links=com.streamsets.pipeline.stage.executor.databricks.DatabricksJobLauncherDExecutor,com.streamsets.pipeline.stage.processor.controlHub.ControlHubApiDProcessor,com.streamsets.pipeline.stage.executor.emailexecutor.EmailDExecutor,com.streamsets.pipeline.stage.processor.startJob.StartJobDProcessor,com.streamsets.pipeline.stage.origin.startJob.StartJobDSource,com.streamsets.pipeline.stage.processor.waitForJobCompletion.WaitForJobCompletionDProcessor,com.streamsets.pipeline.stage.destination.sdcipc.StatsSdcIpcDTarget,com.streamsets.pipeline.stage.destination.cassandra.CassandraDTarget,com.streamsets.pipeline.stage.processor.waitForPipelineCompletion.WaitForPipelineCompletionDProcessor,com.streamsets.pipeline.stage.processor.geolocation.GeolocationDProcessor,com.streamsets.pipeline.stage.origin.sdcipcwithbuffer.SdcIpcWithDiskBufferDSource,com.streamsets.pipeline.stage.origin.ipctokafka.SdcIpcToKafkaDSource,com.streamsets.pipeline.stage.origin.jdbc.SapHanaDSource,com.streamsets.pipeline.stage.origin.httpserver.nifi.NiFiHttpServerDPushSource,com.streamsets.pipeline.stage.origin.grpcclient.GrpcClientDSource
  # System and user stage libraries whitelists and blacklists
  #
  # If commented out all stagelibraries directories are used.
  #
  # Given 'system' or 'user', only whitelist or blacklist can be set, if both are set the Data Collector will fail to start
  #
  # Specify stage library directories separated by commas
  #
  # The MapR stage libraries are disabled as they require manual installation step. Use the setup-mapr script to enable
  # the desired MapR stage library.
  #
  # It's important to keep the blacklist and whitelist properties on a single line, otherwise CSD's control.sh script and
  # setup-mapr script will not work properly.
  #
  #system.stagelibs.whitelist=
  system.stagelibs.blacklist=streamsets-datacollector-mapr_6_0-lib,streamsets-datacollector-mapr_6_0-mep4-lib,streamsets-datacollector-mapr_6_0-mep5-lib,streamsets-datacollector-mapr_6_1-lib,streamsets-datacollector-mapr_6_1-mep6-lib
  #
  #user.stagelibs.whitelist=
  #user.stagelibs.blacklist=

  # Stage Classpath Validation
  #
  # Uncomment to disable best effort validation of each stage library classpath to detect known issues with
  # colliding dependencies (such as conflicting versions of the same dependency, ...). Result of the validation
  # is by default only printed to log.
  #stagelibs.classpath.validation.enable=false
  #
  # By default the validation result is only logged. Uncomment to prevent Data Collector to start if classpath of any
  # stage library is not considered valid.
  #stagelibs.classpath.validation.terminate=true

  # Health Inspector Configuration
  #
  # Configuration options specific to alter behavior of health inspector.
  #
  #health_inspector.network.host=www.streamsets.com

  #
  # Additional configuration files to include in to the configuration.
  # Value of this property is the name of the configuration file separated by commas.
  #
  config.includes=dpm.properties,vault.properties,credential-stores.properties

  #
  # Record sampling configurations indicate the size of the subset (sample set) that must be chosen from a population (of records).
  # Default configuration values indicate the sampler to select 1 out of 10000 records
  #
  # For better performance simplify the fraction ( sdc.record.sampling.sample.size / sdc.record.sampling.population.size )
  # i.e., specify ( 1 / 40 ) instead of ( 250 / 10000 ).
  sdc.record.sampling.sample.size=1
  sdc.record.sampling.population.size=10000

  #
  # Pipeline state are cached for faster access.
  # Specifies the maximum number of pipeline state entries the cache may contain.
  store.pipeline.state.cache.maximum.size=100

  # Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration
  # has elapsed after the entry's creation, the most recent replacement of its value, or its last access.
  # In minutes
  store.pipeline.state.cache.expire.after.access=10

  # pipeline store module
  # dataStore module store in local, objectStore module store in minio
  web.pipeline.store.module=objectStore
  web.pipeline.minio.store.endpoint=http://10.206.68.1:30290
  web.pipeline.minio.store.accessKey=admin
  web.pipeline.minio.store.secretKey=12345678
  web.pipeline.minio.store.bucket.name=data-thread

# support-bundle-redactor.json
support-bundle-redactor.json: |-
  {
    "version": 1,
    "rules": [
      {
        "description": "Generic password in configuration files.",
        "trigger": "password",
        "search": "password=[^\"' ]*",
        "replace": "password=REDACTED"
      }, {
        "description": "DPM Authorization token",
        "trigger": "dpm.appAuthToken",
        "search": "dpm.appAuthToken=[^\"' ]*",
        "replace": "dpm.appAuthToken=REDACTED"
      }, {
        "description": "LDAP Password",
        "trigger": "bindPassword",
        "search": "bindPassword=[^\"' ]*",
        "replace": "bindPassword=REDACTED"
      }, {
        "description": "SDC Sensitive Configs",
        "trigger": ".password=",
        "search": "^(https.cluster.keystore.password|https.keystore.password|https.truststore.password|https.cluster.truststore.password|xmail.password|lineage.publisher.navigator.config.password)=.*$",
        "replace": "$1=REDACTED"
      }
    ]
  }

# vault.properties
vault.properties: |-
  #
  # Copyright 2017 StreamSets Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  #

  #
  # Vault EL functions, vault:read() and vault:readWithDelay(), are deprecated.
  # Use instead the CredentialStore EL functions with a Vault implementation.
  #
  # Vault configuration should be set in the credential-stores.properties file. Previous Vault configuration property
  # names must be added to the credentials-store.properties file prefixed with 'credentialStore.vault.'.
  #
  #
  # For backwards compatibility, you can specify with the following property, the ID of the Vault CredentialStore
  # you want to be used with the deprecated functions.
  #

  #vaultEL.credentialStore.id=vault
