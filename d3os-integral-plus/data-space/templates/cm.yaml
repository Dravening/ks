apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "data-space.fullname" . }}
  labels:
    app: {{ template "data-space.fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
data:
  application-token.txt: ''
  basic-realm.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n#The format is\r\n#  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]\r\n#\r\n# Supported roles are: admin, manager, creator, guest\r\n#\r\n# 'user' must always be present\r\n#\r\n# Prefix with 'group:' for group information for the user.\r\n#\r\n\r\n# BASIC authentication, password is same as user name\r\nadmin:   MD5:21232f297a57a5a743894a0e4a801fc3,user,admin\r\nguest:   MD5:084e0343a0486ff05530df6c705c8bb4,user,guest\r\ncreator: MD5:ee2433259b0fe399b40e81d2c98a38b6,user,creator\r\nmanager: MD5:1d0258c2440a8d19e716292b231e3190,user,manager\r\nuser1:   MD5:24c9e15e52afc47c225b757e7bee1f9d,user,manager,creator,group:dev\r\nuser2:   MD5:7e58d63b60197ceb55a1c487989a3720,user,manager,creator,group:dev\r\nuser3:   MD5:92877af70a45fd6a2ed7fe81e1236b78,user,manager,creator,group:test\r\nuser4:   MD5:3f02ebe3d7929b091e3d8ccfde2f3bc6,user,manager,creator,group:test\r\n\r\n#\r\n# To compute the MD5 run the following command:\r\n#\r\n# OSX:\r\n#      $ echo -n \"<password>\" | md5\r\n#\r\n# Linux:\r\n#      $ echo -n \"<password>\" | md5sum\r\n#\r\n"
  credential-stores.properties: "#\r\n# Copyright 2020 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n# Use this file to enable the use of credential stores with Data Collector.\r\n\r\n# IMPORTANT: This file includes a set of properties for each credential store type.\r\n# Property names include the default credential store IDs: jks,aws,azure,cyberark,thycotic,vault.\r\n# When you use custom IDs, you must update the corresponding property names.\r\n\r\n# To use multiple credential stores of the same type, make sure each credential store\r\n# has a set of related properties defined. Make sure the property names include\r\n# the appropriate credential store ID.\r\n\r\n################################################\r\n#       Data Collector Credential Stores       #\r\n################################################\r\n\r\n# Defines the credential stores for Data Collector to use. Specify a comma-separated list\r\n# of unique credential store IDs.\r\n#credentialStores=jks,aws,azure,cyberark,thycotic,vault\r\n\r\n################################################\r\n# Java Keystore Credential Store Configuration #\r\n################################################\r\n\r\n# The following properties are for a Java keystore credential store that uses the 'jks'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property\r\n# above, replace 'jks' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'jks' credential store\r\n# Update 'jks' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.jks.def=streamsets-datacollector-jks-credentialstore-lib::com_streamsets_datacollector_credential_javakeystore_JavaKeyStoreCredentialStore\r\n\r\n# A Java keystore credential store can be of type JCEKS or PKCS12\r\ncredentialStore.jks.config.keystore.type=PKCS12\r\n\r\n# The location of the Java keystore. Specify an absolute path or a path relative to the\r\n# $SDC_CONF directory.\r\ncredentialStore.jks.config.keystore.file=jks-credentialStore.pkcs12\r\n\r\n# The password to access the Java keystore\r\ncredentialStore.jks.config.keystore.storePassword=changeIt\r\n\r\n# The minimum refresh millis used to reload the keystore file\r\n#credentialStore.jks.config.keystore.file.min.refresh.millis=10000\r\n\r\n############################################################\r\n#    AWS Secrets Manager Credential Store Configuration    #\r\n############################################################\r\n\r\n# The following properties are for an AWS Secrets Manager credential store that uses the 'aws'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property,\r\n# replace 'aws' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'aws' credential store\r\n# Update 'aws' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.aws.def=streamsets-datacollector-aws-secrets-manager-credentialstore-lib::com_streamsets_datacollector_credential_aws_secrets_manager_AWSSecretsManagerCredentialStore\r\n\r\n# Default name-key separator for the name parameter in credential functions\r\ncredentialStore.aws.config.nameKey.separator=&\r\n\r\n# AWS Region\r\ncredentialStore.aws.config.region=<MUST BE SET>\r\n\r\n# It must be: accessKeys or instanceProfile\r\ncredentialStore.aws.config.security.method=accessKeys\r\n\r\n# AWS access key\r\ncredentialStore.aws.config.access.key=<MUST BE SET IF ACCESS KEYS IS USED AS A SECURITY METHOD>\r\n\r\n# AWS secret key\r\ncredentialStore.aws.config.secret.key=<MUST BE SET IF ACCESS KEYS IS USED AS A SECURITY METHOD>\r\n\r\n# Secrets cache max size\r\n# Maximum number of secrets to cache locally\r\ncredentialStore.aws.config.cache.max.size=1024\r\n\r\n# Secrets cache TTL\r\n# The number of milliseconds that a cached secret is considered valid before requiring a refresh\r\n# The default is equivalent to 1 hour\r\ncredentialStore.aws.config.cache.ttl.millis=3600000\r\n\r\n# Requires a group secret for each secret\r\ncredentialStore.aws.config.enforceEntryGroup=false\r\n\r\n########################################################\r\n#    Azure Key Vault Credential Store Configuration    #\r\n########################################################\r\n\r\n# The following properties are for an Azure Key Vault credential store that uses the 'azure'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property,\r\n# replace 'azure' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'azure' credential store\r\n# Update 'azure' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.azure.def=streamsets-datacollector-azure-keyvault-credentialstore-lib::com_streamsets_datacollector_credential_azure_keyvault_AzureKeyVaultCredentialStore\r\n\r\n# Credential refresh interval\r\n# How long a credential can be cached locally before fetching it again from Azure Key Vault.\r\ncredentialStore.azure.config.credential.refresh.millis=30000\r\n\r\n# Credential retry interval\r\n# How long to wait before retrying to fetch a credential from Azure Key Vault in case of errors.\r\n# This retry delay is not blocking. Locally, it will fail immediately.\r\ncredentialStore.azure.config.credential.retry.millis=15000\r\n\r\n# Azure Key Vault credential provider URL\r\n# This property must be set.\r\n# credentialStore.azure.config.vault.url=https://<YOUR_KEY_VAULT>.vault.azure.net/\r\n\r\n# Azure Key Vault client ID for this Data Collector\r\n#credentialStore.azure.config.client.id=<MUST BE SET>\r\n\r\n# Azure Key Vault client key for this Data Collector\r\n#credentialStore.azure.config.client.key=<MUST BE SET>\r\n\r\n# ERequires a group secret for each secret\r\ncredentialStore.azure.config.enforceEntryGroup=false\r\n\r\n#################################################\r\n#    CyberArk Credential Store Configuration    #\r\n#################################################\r\n\r\n# The following properties are for a CyberArk credential store that uses the 'cyberark'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property,\r\n# replace 'cyberark' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'cyberark' credential store\r\n# Update 'cyberark' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.cyberark.def=streamsets-datacollector-cyberark-credentialstore-lib::com_streamsets_datacollector_credential_cyberark_CyberArkCredentialStore\r\n\r\n# Credential refresh interval\r\n# How long a credential can be cached locally before fetching it again from CyberArk.\r\n#credentialStore.cyberark.config.credential.refresh.millis=30000\r\n\r\n# Credential retry interval\r\n# How long to wait before retrying to fetch a credential from CyberArk in case of errors.\r\n# This retry delay is not blocking. Locally, it will fail immediately.\r\n#credentialStore.cyberark.config.credential.retry.millis=15000\r\n\r\n# Connector type to CyberArk\r\n# Currently 'webservices' is the only supported connector\r\n#credentialStore.cyberark.config.connector=webservices\r\n\r\n##############################################################\r\n#     CyberArk Credential Store Web Service Configuration    #\r\n##############################################################\r\n\r\n# CyberArk Central Credential Provider credential retrieval web service URL\r\ncredentialStore.cyberark.config.ws.url=https://<HOST>:<PORT>/AIMWebService/api/Accounts\r\n\r\n# CyberArk application ID for this Data Collector\r\ncredentialStore.cyberark.config.ws.appId=<MUST BE SET>\r\n\r\n# Maximum number of concurrent web service calls to CyberArk\r\n#credentialStore.cyberark.config.ws.maxConcurrentConnections=10\r\n\r\n# HTTP connection inactivity check\r\n#credentialStore.cyberark.config.ws.validateAfterInactivity.millis=60000\r\n\r\n# TCP and HTTP connection timeout\r\n#credentialStore.cyberark.config.ws.connectionTimeout.millis=10000\r\n\r\n# Default separator for CyberArk safe, folder, object name, and object element used in the\r\n# name parameter in credential functions.\r\n#credentialStore.cyberark.config.ws.nameSeparator=&\r\n\r\n# HTTP authentication mechanism used by CyberArk Central Credential Provider web services\r\n# Possible values: none, basic, digest\r\n#credentialStore.cyberark.config.ws.http.authentication=none\r\n\r\n# User name when using basic or digest authentication\r\n#credentialStore.cyberark.config.ws.http.authentication.user=\r\n\r\n# Password when using basic or digest authentication\r\n#credentialStore.cyberark.config.ws.http.authentication.password=\r\n\r\n# When using HTTPS and the server certificate is not signed by a public CA, a truststore\r\n# with the public certificate must be available in this truststore file, or in the JDK default truststore.\r\n# Specify an absolute path or a path relative to the $SDC_CONF directory.\r\n#credentialStore.cyberark.config.ws.truststoreFile=\r\n\r\n# The password to access the truststore file\r\n#credentialStore.cyberark.config.ws.truststorePassword=\r\n\r\n# HTTPS supported protocols\r\n#credentialStore.cyberark.config.ws.supportedProtocols=TLSv1.2\r\n\r\n# Determines if the hostname of the CyberArk Central Credential Provider web service should be\r\n# verified against the domain defined in the HTTPS certificate.\r\n#credentialStore.cyberark.config.ws.hostnameVerifier.skip=false\r\n\r\n# When using HTTPS and the CyberArk Central Credential Provider web service is configured to require client side\r\n# certificates, the client certificate must be available in this keystore file, or in the JDK default truststore.\r\n# Specify an absolute path or a path relative to the $SDC_CONF directory.\r\n#credentialStore.cyberark.config.ws.keystoreFile=\r\n\r\n# The password to access the keystore file\r\n#credentialStore.cyberark.config.ws.keystorePassword=\r\n\r\n# The password to access the certificate within the keystore file\r\n#credentialStore.cyberark.config.ws.keyPassword=\r\n\r\n# Requires a group secret for each secret\r\ncredentialStore.cyberark.config.enforceEntryGroup=false\r\n\r\n########################################################\r\n#    Hashicorp Vault Credential Store Configuration    #\r\n########################################################\r\n\r\n# The following properties are for a Hashicorp Vault credential store that uses the 'vault'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property,\r\n# replace 'vault' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'vault' credential store\r\n# Update 'vault' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.vault.def=streamsets-datacollector-vault-credentialstore-lib::com_streamsets_datacollector_credential_vault_VaultCredentialStore\r\n\r\n# Default path-key separator for the name parameter in credential functions\r\ncredentialStore.vault.config.pathKey.separator=&\r\n\r\n# URL of the Vault server to connect to\r\ncredentialStore.vault.config.addr=http://localhost:8200\r\n\r\n# AppRole mode (recommended)\r\ncredentialStore.vault.config.role.id=\r\ncredentialStore.vault.config.secret.id=${file(\"vault-secret-id\")}\r\n\r\n#\r\n# The Vault User ID is generated by hashing the MAC address belonging to the network interface assigned\r\n# the IP address of hostname -f. It can also be retrieved by the show-vault-id command of the\r\n# StreamSets executable.\r\n#\r\n\r\n# Data Collector authenticates with Vault using the AppId authentication backend. The app-id must be specified below.\r\n# credentialStore.vault.config.app.id=\r\n\r\n# Optional Settings\r\n\r\n# Supported KV Secret Engine version 1 by default. Possible values: 1 or 2.\r\ncredentialStore.vault.config.version=1\r\n\r\n# Define namespaces for Vault Enterprise\r\n#credentialStore.vault.config.namespace=\r\n\r\n# The renewal interval must be shorter than the shortest lease issued by Vault including auth tokens.\r\ncredentialStore.vault.config.lease.renewal.interval.sec=60\r\ncredentialStore.vault.config.lease.expiration.buffer.sec=120\r\ncredentialStore.vault.config.open.timeout=0\r\ncredentialStore.vault.config.proxy.address=\r\ncredentialStore.vault.config.proxy.port=8080\r\ncredentialStore.vault.config.proxy.username=\r\ncredentialStore.vault.config.proxy.password=\r\ncredentialStore.vault.config.read.timeout=0\r\ncredentialStore.vault.config.ssl.enabled.protocols=TLSv1.2,TLSv1.3\r\ncredentialStore.vault.config.ssl.truststore.file=\r\ncredentialStore.vault.config.ssl.truststore.password=\r\ncredentialStore.vault.config.ssl.verify=true\r\ncredentialStore.vault.config.ssl.timeout=0\r\ncredentialStore.vault.config.timeout=0\r\n\r\n# Requires a group secret for each secret\r\ncredentialStore.vault.config.enforceEntryGroup=false\r\n\r\n#####################################################################\r\n#    Thycotic Secret Server Credential Store Configuration          #\r\n#####################################################################\r\n\r\n# The following properties are for an Thycotic Secret Server credential store that uses the 'thycotic'\r\n# default credential store ID. If you specified a custom ID in the credentialStores property,\r\n# replace 'thycotic' in the property names with the custom ID.\r\n\r\n# Defines the implementation of the 'thycotic' credential store.\r\n# Update 'thycotic' in the property name as needed, but do not change the definition of this property.\r\ncredentialStore.thycotic.def=streamsets-datacollector-thycotic-credentialstore-lib::com_streamsets_datacollector_credential_thycotic_ThycoticCredentialStore\r\n\r\n# Thycotic Secret Server URL. Use the following format: https://<host name>:<port number>.\r\n# Use HTTPS to avoid unencrypted communication.\r\ncredentialStore.thycotic.config.url=<MUST BE SET>\r\n\r\n# User name to connect to Thycotic Secret Server\r\ncredentialStore.thycotic.config.username=<MUST BE SET>\r\n\r\n# Password to connect to Thycotic Secret Server\r\ncredentialStore.thycotic.config.password=${file(\"thycotic-secret-password\")}\r\n\r\n# Cache expiration time\r\ncredentialStore.thycotic.config.credential.cache.inactivityExpiration.seconds=1800\r\n\r\n# Credential refresh interval\r\n# How long a credential can be cached locally before fetching it again from Thycotic Secret Server.\r\ncredentialStore.thycotic.config.credential.refresh.seconds=30000\r\n\r\n# Credential retry interval\r\n# How long to wait before retrying to fetch a credential from Thycotic Secret Server in the case of an error.\r\ncredentialStore.thycotic.config.credential.retry.seconds=15000\r\n\r\n# Buffer for expiring auth tokens. Data Collector renews tokens that expire in less than\r\n# the specified number of seconds. Default is 1201.\r\ncredentialStore.thycotic.config.token.expiration.buffer.seconds=1201\r\n\r\n# SSL/TLS-enabled protocols. Versions TLSv1.2 or later are recommended. Default is TLSv1.2,TLSv1.3\r\ncredentialStore.thycotic.config.ssl.enabled.protocols=TLSv1.2,TLSv1.3\r\n\r\n# Path to a Java truststore file. Required when using a private CA or certificates not trusted\r\n# by the Java default truststore.\r\ncredentialStore.thycotic.config.ssl.truststore.file=\r\n\r\n# Password for the truststore file\r\ncredentialStore.thycotic.config.ssl.truststore.password=\r\n\r\n# Whether to verify that the Thycotic server hostname matches its certificate.\r\n# Default is true. False is not recommended.\r\ncredentialStore.thycotic.config.ssl.verify=true\r\n\r\n# Timeout for the SSL/TLS handshake in milliseconds. Default is 0 for no limit.\r\ncredentialStore.thycotic.config.ssl.timeout=0\r\n\r\n# Separator to use for the Thycotic Secret Server secret ID and field name values in the\r\n# credential name argument used in credential functions.\r\ncredentialStore.thycotic.config.nameSeparator=-\r\n\r\n# Milliseconds to wait for data before timing out.\r\n# Default is 0 for no limit.\r\ncredentialStore.thycotic.config.read.timeout=0\r\n\r\n# Timeout to establish an HTTP connection to Thycotic Secret Server in milliseconds.\r\n# Default is 0 for no limit.\r\ncredentialStore.thycotic.config.open.timeout=0\r\n\r\n# Timeout in milliseconds to read from Thycotic Secret Server after a connection has been established.\r\n# Default is 0 for no limit.\r\ncredentialStore.thycotic.config.timeout=0\r\n\r\n# Requires a group secret for each secret\r\ncredentialStore.thycotic.config.enforceEntryGroup=false\r\n"
  database-realm.properties: "jdbcdriver=com.mysql.jdbc.Driver\r\nurl=jdbc:mysql://mysql-space.{{ .Release.Namespace }}:3306/datathread\r\nusername=d3os\r\npassword=d3os@cosmoplat\r\nusertable=users\r\nusertablekey=id\r\nusertableuserfield=username\r\nusertablepasswordfield=password\r\nroletable=roles\r\nroletablekey=id\r\nroletablerolefield=rolename\r\nuserroletable=user_role\r\nuserroletableuserkey=user_id\r\nuserroletablerolekey=role_id\r\nremoteurl=http://login-center.{{ .Release.Namespace }}.svc:8081\r\nloginflag=remote\r\n"
  digest-realm.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n#The format is\r\n#  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]\r\n#\r\n# Supported roles are: admin, manager, creator, guest\r\n#\r\n# 'user' must always be present\r\n#\r\n# Prefix with 'group:' for group information for the user.\r\n#\r\n\r\n# DIGEST authentication, password is same as user name\r\nadmin:   MD5:184b0de86a7c6e86924b5978c97d6969,user,admin\r\nguest:   MD5:bb1b090606eb8f94cd0f03b3f37f1cf0,user,guest\r\ncreator: MD5:e71e2f1c732f81a723285369a4e5ca89,user,creator\r\nmanager: MD5:97500f290705dbbe5be2889bcbdd9ed0,user,manager\r\nuser1:   MD5:d06240e76c12549a2efe9262f2ef2cd8,user,manager,creator,group:dev\r\nuser2:   MD5:791644a3c770ad2cf72c6c6947f8c1d6,user,manager,creator,group:dev\r\nuser3:   MD5:8a1f9771c26853e0bbeb178519d9d4a9,user,manager,creator,group:test\r\nuser4:   MD5:bd63a028da4987d08da7ff5a6026888c,user,manager,creator,group:test\r\n\r\n#\r\n# To compute the MD5 run the following command:\r\n#\r\n# OSX:\r\n#\r\n#      $ echo -n \"<username>:<realm>:<password>\" | md5\r\n#\r\n# Linux:\r\n#\r\n#      $ echo -n \"<username>:<realm>:<password>\" | md5sum\r\n#\r\n"
  dpm-url.txt: "http://localhost:18631\r\n"
  dpm.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n#\r\n# Control Hub Enabled\r\n# If true HTTP Authentication for Data Collector will be configured to use Control Hub SSO Authentication.\r\n#\r\ndpm.enabled=false\r\n\r\n#\r\n# Base URL of the Remote Service\r\n# In a real deployment the security service must be encrypted (HTTPS)\r\n#\r\ndpm.base.url=@dpm-url.txt@\r\n\r\n#\r\n# Registration attempts.\r\n# There is an exponential backoff that starts with 2 seconds and maxes out at 16 seconds.\r\n#\r\ndpm.registration.retry.attempts=5\r\n\r\n#\r\n# Frequency of validation of user and app authentication tokens.\r\n# As part of this validation all information about the principal is refreshed.\r\n#\r\ndpm.security.validationTokenFrequency.secs=60\r\n\r\n#\r\n# Application Token\r\n#\r\ndpm.appAuthToken=@application-token.txt@\r\n\r\n#\r\n# Labels for this Data Collector to report the Control Hub\r\n#\r\ndpm.remote.control.job.labels=all\r\n\r\n#\r\n# Data Collector Ping Frequency to Control Hub (in milliseconds)\r\n#\r\ndpm.remote.control.ping.frequency=5000\r\n\r\n#\r\n# App to send remote control events\r\n#\r\ndpm.remote.control.events.recipient=jobrunner-app\r\n\r\n#\r\n# Apps to send Data Collector Process metrics (CPU Load and Heap Memory Usage)\r\n#\r\ndpm.remote.control.process.events.recipients=jobrunner-app,timeseries-app\r\n\r\n#\r\n# Frequency to send pipeline status events (all remote pipelines and local running pipelines) and\r\n# Data Collector process metrics like CPU load and heap memory usage\r\n#\r\ndpm.remote.control.status.events.interval = 60000\r\n\r\n\r\ndpm.remote.deployment.id=\r\n\r\n#\r\n# Indicates if the redirection to Control Hub SSO is done using HTML META refresh.\r\n# This is useful for environment that rewrite redirect headers.\r\n#\r\nhttp.meta.redirect.to.sso=false\r\n\r\n\r\n# Uncomment to use 'user' as hadoop proxy user from the full user name 'user@org'. Below setting\r\n# only takes effect when sdc is control hub enabled and stage impersonation is set to true\r\n#\r\n#dpm.alias.name.enabled=true\r\n"
  email-password.txt: password
  form-realm.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n#The format is\r\n#  <user>: MD5:<password>[,user,<role>,<role>,...,<group:group1>,<group:group2>,....]\r\n#\r\n# Supported roles are: admin, manager, creator, guest\r\n#\r\n# 'user' must always be present\r\n#\r\n# Prefix with 'group:' for group information for the user.\r\n#\r\n\r\n# FORM authentication, password is same as user name\r\nadmin:   MD5:21232f297a57a5a743894a0e4a801fc3,user,admin\r\nguest:   MD5:084e0343a0486ff05530df6c705c8bb4,user,guest\r\ncreator: MD5:ee2433259b0fe399b40e81d2c98a38b6,user,creator\r\nmanager: MD5:1d0258c2440a8d19e716292b231e3190,user,manager\r\nuser1:   MD5:24c9e15e52afc47c225b757e7bee1f9d,user,manager,creator,group:dev\r\nuser2:   MD5:7e58d63b60197ceb55a1c487989a3720,user,manager,creator,group:dev\r\nuser3:   MD5:92877af70a45fd6a2ed7fe81e1236b78,user,manager,creator,group:test\r\nuser4:   MD5:3f02ebe3d7929b091e3d8ccfde2f3bc6,user,manager,creator,group:test\r\n\r\n\r\n#\r\n# To compute the MD5 run the following command:\r\n#\r\n# OSX:\r\n#      $ echo -n \"<password>\" | md5\r\n#\r\n# Linux:\r\n#      $ echo -n \"<password>\" | md5sum\r\n#\r\n"
  jdbc.properties: "jdbc.driver=com.mysql.cj.jdbc.Driver\r\njdbc.url=jdbc:mysql://mysql-space.{{ .Release.Namespace }}:3306/datathread?useSSL=false&autoReconnect=true&serverTimezone=GMT%2B8&rewriteBatchedStatements=true\r\njdbc.username=d3os\r\njdbc.password=d3os@cosmoplat\r\ndata.separate.base.on.jdbc=separate"
  keystore-password.txt: "password\r\n"
  ldap-bind-password.txt: "password\r\n"
  ldap-login.conf: "ldap {\r\n  com.streamsets.datacollector.http.LdapLoginModule required\r\n  debug=\"false\"\r\n  useLdaps=\"false\"\r\n  useStartTLS=\"false\"\r\n  contextFactory=\"com.sun.jndi.ldap.LdapCtxFactory\"\r\n  hostname=\"#### LDAP HOST ####\"\r\n  port=\"389\"\r\n  bindDn=\"cn=Directory Manager\"\r\n  bindPassword=\"@ldap-bind-password.txt@\"\r\n  forceBindingLogin=\"false\"\r\n  userBaseDn=\"ou=people,dc=alcatel\"\r\n  userIdAttribute=\"uid\"\r\n  userPasswordAttribute=\"userPassword\"\r\n  userObjectClass=\"inetOrgPerson\"\r\n  userFilter=\"uid={user}\"\r\n  roleBaseDn=\"ou=groups,dc=example,dc=com\"\r\n  roleNameAttribute=\"cn\"\r\n  roleMemberAttribute=\"member\"\r\n  roleObjectClass=\"groupOfNames\"\r\n  roleFilter=\"member={dn}\";\r\n};"
  remote-realm.properties: "remote.url=http://login-center.{{ .Release.Namespace }}.svc:8081\r\nlogin.flag=remote\r\nremote.user.create.interface=/center/sys-user/register\r\nremote.user.get.info.interface=/center/sys-user/getUserInfo\r\nremote.user.login.interface=/center/login\r\nremote.user.reset.password.interface=/center/sys-user/passwordReset\r\nremote.user.change.password.interface=/center/sys-user/operatePassword\r\nremote.user.update.interface=/center/sys-user/operateUser\r\nremote.user.list.query.interface=/center/sys-user/getUserList\r\nremote.user.group.list.query.interface=/center/sys-company/checkCompany\r\n"
  sdc-log4j.properties: "# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n# /dev/null appender\r\nlog4j.appender.null=org.apache.log4j.FileAppender\r\nlog4j.appender.null.File=/dev/null\r\nlog4j.appender.null.layout=org.apache.log4j.PatternLayout\r\nlog4j.appender.null.layout.ConversionPattern=null\r\n\r\n# <stdout> appender\r\nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender\r\nlog4j.appender.stdout.Target=System.out\r\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout\r\nlog4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [runner:%X{s-runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n\r\n\r\n# sdc.log appender\r\nlog4j.appender.streamsets=org.apache.log4j.RollingFileAppender\r\nlog4j.appender.streamsets.MaxFileSize=256MB\r\nlog4j.appender.streamsets.MaxBackupIndex=10\r\nlog4j.appender.streamsets.File=${sdc.log.dir}/sdc.log\r\nlog4j.appender.streamsets.Append=true\r\nlog4j.appender.streamsets.layout=org.apache.log4j.PatternLayout\r\nlog4j.appender.streamsets.layout.ConversionPattern=%d{ISO8601} [user:%X{s-user}] [pipeline:%X{s-entity}] [runner:%X{s-runner}] [thread:%t] [stage:%X{s-stage}] %-5p %c{1} - %m%n\r\n# log4j.appender.file3=org.apache.log4j.DailyRollingFileAppender\r\n# log4j.appender.file3.Append=true\r\n# log4j.appender.file3.DatePattern='_'yyyy-MM-dd\r\n# log4j.appender.file3.File=/opt/data_process/logs/edc_report.log\r\n# log4j.appender.file3.Threshold=INFO\r\n# log4j.appender.file3.Encoding=UTF-8\r\n# log4j.appender.file3.layout=org.apache.log4j.PatternLayout\r\n# log4j.appender.file3.layout.ConversionPattern=%p %d{yyyy-MM-dd HH\\:mm\\:ss} %C.%M(%L) | %m %n\r\n\r\nlog4j.rootLogger=INFO, streamsets\r\nlog4j.logger.com.streamsets=INFO\r\nlog4j.logger.org.eclipse.jetty=WARN\r\nlog4j.logger.com.amazonaws.services.kinesis.clientlibrary.lib.worker.SequenceNumberValidator=WARN\r\nlog4j.logger.org.apache.kafka=WARN\r\n# log4j.logger.com.datastax.driver.core.QueryLogger.SLOW=DEBUG\r\nlog4j.logger.com.datastax.driver.core.QueryLogger.SLOW=INFO\r\n"
  sdc-security.policy: "// Copyright 2017 StreamSets Inc.\r\n//\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n//     http://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n// Unless required by applicable law or agreed to in writing, software\r\n// distributed under the License is distributed on an \"AS IS\" BASIS,\r\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n// See the License for the specific language governing permissions and\r\n// limitations under the License.\r\n\r\n// StreamSets Data Collector Policy File\r\n\r\n// StreamSets code base:\r\ngrant codebase \"file://${sdc.bootstrapLib.dir}/-\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.rootLib.dir}/*\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.apiLib.dir}/*\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.asterClientLib.dir}/*\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.containerLib.dir}/*\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.librariesExtras.dir}/-\" {\r\n  permission java.security.AllPermission;\r\n};\r\n// StreamSets stage libraries code base:\r\ngrant codebase \"file://${sdc.libraries.dir}/-\" {\r\n  permission java.security.AllPermission;\r\n};\r\ngrant codebase \"file://${sdc.userLibs.dir}/streamsets-datacollector-dev-lib/-\" {\r\n  permission java.security.AllPermission;\r\n};\r\n// Groovy will parse files in a different context, so we need to grant it additional privileges\r\ngrant codebase \"file:/groovy/script\" {\r\n  permission java.lang.RuntimePermission \"getClassLoader\";\r\n};\r\n\r\n// For details on how to grant specific permissions, refer to the Java Permissions Documentation:\r\n//   http://docs.oracle.com/javase/7/docs/technotes/guides/security/permissions.html\r\n\r\n// User stage libraries code base:\r\ngrant codebase \"file://${sdc.userLibs.dir}/-\" {\r\n  permission java.util.PropertyPermission \"*\", \"read\";\r\n  permission java.lang.RuntimePermission \"accessDeclaredMembers\";\r\n  permission java.lang.reflect.ReflectPermission \"suppressAccessChecks\";\r\n  permission java.io.FilePermission \"${sdc.dist.dir}/user-libs/streamsets-datacollector-dev-lib/lib/*\", \"read\";\r\n};\r\n\r\n// For JARs to be available to all stage libraries\r\ngrant codebase \"file://${sdc.libsCommon.dir}/-\" {\r\n  permission java.security.AllPermission;\r\n};\r\n"
  sdc.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n# HTTP configuration\r\n\r\n# The base URL of Data Collector, used to create email alert messages.\r\n# If not set http://<hostname>:<http.port> is used\r\n# <hostname> is either taken from http.bindHost or resolved using\r\n# 'hostname -f' if not configured.\r\n#sdc.base.http.url=http://<hostname>:<port>\r\n\r\n# Hostname or IP address that Data Collector will bind to.\r\n# Default is 0.0.0.0 that will bind to all interfaces.\r\n#http.bindHost=0.0.0.0\r\n\r\n# Maximum number of HTTP servicing threads.\r\n#http.maxThreads=200\r\n\r\n# The port Data Collector uses as an HTTP endpoint.\r\n# If different from -1, the Data Collector will run on this port\r\n# If 0, the Data Collector will pick up a random port\r\n# If the https.port is different that -1 or 0 and http.port is different than -1 or 0, the HTTP endpoint\r\n# will redirect to the HTTPS endpoint.\r\nhttp.port=18630\r\n\r\n# HTTPS configuration\r\n\r\n# TThe port Data Collector uses as an HTTPS endpoint.\r\n# If different from -1, the Data Collector will run over SSL on this port\r\n# If 0, the Data Collector will use a random port\r\nhttps.port=-1\r\n\r\n# Enables HTTP/2 support for the Data Collector UI/REST API. If you are using any clients\r\n# that do not support ALPN for protocol negotiation, leave this option disabled.\r\nhttp2.enable=false\r\n\r\n# Reverse Proxy / Load Balancer configuration\r\n\r\n# Data Collector will handle X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port\r\n# headers issued by a reverse proxy such as HAProxy, ELB, nginx when set to true.\r\n# Set to true when hosting Data Collector behind a reverse proxy / load balancer.\r\nhttp.enable.forwarded.requests=false\r\n\r\n# Java keystore file, in the Data Collector 'etc/' configuration directory\r\nhttps.keystore.path=keystore.jks\r\n\r\n# Password for the keystore file,\r\n# By default, the password is loaded from 'keystore-password.txt'\r\n# in the Data Collector 'etc/' configuration directory\r\nhttps.keystore.password=${file(\"keystore-password.txt\")}\r\n\r\n# Path to keystore file on the worker node. This should always be an absolute location\r\nhttps.cluster.keystore.path=/opt/security/jks/sdc-keystore.jks\r\n\r\n# Password for keystore file on the worker node\r\nhttps.cluster.keystore.password=${file(\"/opt/security/jks/keystore-password.txt\")}\r\n\r\n# Truststore configs\r\n# By default, if below configs are commented then cacerts from JRE lib directory will be used as truststore\r\n\r\n# Java truststore file on the gateway Data Collector which stores certificates to trust identity of workers\r\n#https.truststore.path=\r\n\r\n# Password for truststore file\r\n#https.truststore.password=\r\n\r\n# Path to truststore file on worker node. This should always be an absolute location\r\n#https.cluster.truststore.path=/opt/security/jks/sdc-truststore.jks\r\n\r\n# Password for truststore file on worker\r\n#https.cluster.truststore.password=${file(\"/opt/security/jks/truststore-password.txt\")}\r\n\r\n# HTTP Session Timeout\r\n# Max period of inactivity, after which the HTTP session is invalidated, in seconds.\r\n# Default value is 86400 seconds (24 hours)\r\n# value -1 means no timeout\r\nhttp.session.max.inactive.interval=86400\r\n\r\n# The authentication for the HTTP endpoint of Data Collector\r\n# Valid values are: 'none', 'basic', 'digest', 'form' or 'aster'\r\n#\r\nhttp.authentication=form\r\n\r\n# Authentication Login Module\r\n# Valid values are: 'file' and 'ldap'\r\n# For 'file', the authentication and role information is read from a property file (etc/basic-realm.properties,\r\n#   etc/digest-realm.properties or etc/form-realm.properties based on the 'http.authentication' value).\r\n# For 'ldap', the authentication and role information is read from a LDAP server\r\n# and LDAP connection information is read from etc/ldap-login.conf.\r\n# http.authentication.login.module=file\r\nhttp.authentication.login.module=remote\r\n\r\n# The realm used for authentication\r\n# A file with the realm name and '.properties' extension must exist in the Data Collector configuration directory\r\n# If this property is not set, the realm name is '<http.authentication>-realm'\r\n#http.digest.realm=local-realm\r\n\r\n# Check the permissions of the realm file should be owner only\r\nhttp.realm.file.permission.check=false\r\n\r\n# LDAP group to Data Collector role mapping\r\n# the mapping is specified as the following pattern:\r\n#    <ldap-group>:<sdc-role>(,<sdc-role>)*(;<ldap-group>:<sdc-role>(,<sdc-role>)*)*\r\n# e.g. Administrator:admin;Manager:manager;DevOP:creator;Tester:guest;\r\nhttp.authentication.ldap.role.mapping=\r\n\r\n# LDAP login module name as present in the JAAS config file.\r\n# If no value is specified, the login module name is assumed to be \"ldap\"\r\nldap.login.module.name=ldap\r\n\r\n# HTTP access control (CORS)\r\nhttp.access.control.allow.origin=*\r\nhttp.access.control.allow.headers=origin, content-type, cache-control, pragma, accept, authorization, x-requested-by, x-ss-user-auth-token, x-ss-rest-call\r\nhttp.access.control.exposed.headers=X-SDC-LOG-PREVIOUS-OFFSET\r\nhttp.access.control.allow.methods=GET, POST, PUT, DELETE, OPTIONS, HEAD\r\n\r\n# Runs the Data Collector within a Kerberos session which is propagated to all stages.\r\n# This is useful for stages that require Kerberos authentication with the services they interact with\r\nkerberos.client.enabled=false\r\n\r\n# The Kerberos principal to use for the Kerberos session.\r\n# It should be a service principal. If the hostname part of the service principal is '_HOST' or '0.0.0.0',\r\n# the hostname will be replaced with the actual complete hostname of Data Collector as advertised by the\r\n# unix command 'hostname -f'\r\nkerberos.client.principal=sdc/_HOST@EXAMPLE.COM\r\n\r\n# The location of the keytab file for the specified principal. If the path is relative, the keytab file will be\r\n# looked for in the Data Collector configuration directory\r\nkerberos.client.keytab=sdc.keytab\r\n\r\n# Maximal batch size for preview\r\npreview.maxBatchSize=1000\r\n# Maximal number of batches for preview\r\npreview.maxBatches=10\r\n# Maximal batch size for pipeline run\r\nproduction.maxBatchSize=50000\r\n\r\n#Specifies the buffer size for Overrun parsers - including JSON, XML and CSV.\r\n#This parameter is specified in bytes, and must be greater than\r\n#1048576 bytes (which is the default size).\r\n#parser.limit=5335040\r\n\r\n#This option determines the number of error records, per stage, that will be retained in memory when the pipeline is\r\n#running. If set to zero, error records will not be retained in memory.\r\n#If the specified limit is reached the oldest records will be discarded to make room for the newest one.\r\nproduction.maxErrorRecordsPerStage=100\r\n\r\n#This option determines the number of pipeline errors that will be retained in memory when the pipeline is\r\n#running. If set to zero, pipeline errors will not be retained in memory.\r\n#If the specified limit is reached the oldest error will be discarded to make room for the newest one.\r\nproduction.maxPipelineErrors=100\r\n\r\n# Max number of concurrent REST calls allowed for the /rest/v1/admin/log endpoint\r\nmax.logtail.concurrent.requests=5\r\n\r\n# Max number of concurrent WebSocket calls allowed\r\nmax.webSockets.concurrent.requests=15\r\n\r\n# Pipeline Sharing / ACLs\r\npipeline.access.control.enabled=false\r\n\r\n# Customize header title for the Data Collector UI\r\n# You can pass any HTML tags here\r\n# Example:\r\n#   For Text  -  <span class=\"navbar-brand\">New Brand Name</span>\r\n#   For Image -  <img src=\"assets/add.png\">\r\nui.header.title=\r\n\r\nui.local.help.base.url=/docs\r\nui.hosted.help.base.url=https://www.streamsets.com/documentation/datacollector/3.22.3-SNAPSHOT/userguide/help\r\n# ui.registration.url=https://registration.streamsets.com/register\r\n# ui.account.registration.url=\r\n\r\nui.refresh.interval.ms=2000\r\nui.jvmMetrics.refresh.interval.ms=4000\r\n\r\n# If set to true, the Data Collector UI will use WebSocket to fetch pipeline status/metrics/alerts. Otherwise, the UI\r\n# will poll every few seconds to get the pipeline status/metrics/alerts.\r\nui.enable.webSocket=true\r\n\r\n# Number of changes supported by undo/redo functionality.\r\n# UI archives pipeline configuration/rules in browser memory to support undo/redo functionality.\r\nui.undo.limit=10\r\n\r\n# Default mode for stage configuration view unless specifically selected by\r\n# the user on the canvas for the pipeline stage.\r\n# Set to ADVANCED to show advanced configurations by default\r\n# ui.default.configuration.view=BASIC\r\n\r\n# SMTP configuration to send alert emails\r\n# All properties starting with 'mail.' are used to create the JavaMail session, supported protocols are 'smtp' & 'smtps'\r\nmail.transport.protocol=smtp\r\nmail.smtp.host=localhost\r\nmail.smtp.port=25\r\nmail.smtp.auth=false\r\nmail.smtp.starttls.enable=false\r\nmail.smtps.host=localhost\r\nmail.smtps.port=465\r\nmail.smtps.auth=false\r\n# If 'mail.smtp.auth' or 'mail.smtps.auth' are to true, these properties are used for the user/password credentials,\r\n# ${file(\"email-password.txt\")} will load the value from the 'email-password.txt' file in the config directory (where this file is)\r\nxmail.username=foo\r\nxmail.password=${file(\"email-password.txt\")}\r\n# FROM email address to use for the messages\r\nxmail.from.address=sdc@localhost\r\n\r\n#Indicates the location where runtime configuration properties can be found.\r\n#Value 'embedded' implies that the runtime configuration properties are present in this file and are prefixed with\r\n#'runtime.conf_'.\r\n#A value other than 'embedded' is treated as the name of a properties file from which the runtime configuration\r\n#properties must be picked up. Note that the properties should not be prefixed with 'runtime.conf_' in this case.\r\nruntime.conf.location=embedded\r\n\r\n# Java Security properties\r\n#\r\n# Any configuration prefixed with 'java.security.<property>' will be set on the static instance java.security.Security\r\n# as part of Data Collector bootstrap process. This will change JVM configuration and should not be used when embedding and running\r\n# multiple Data Collector instances inside the same JVM.\r\n#\r\n# We're explicitly overriding this to zero as JVM will default to -1 if security manager is active.\r\njava.security.networkaddress.cache.ttl=0\r\n\r\n# Security Manager\r\n#\r\n# By default, when Security Manager is enabled, Data Collector will use Java security manager that only follows the specified policy.\r\n\r\n# Enable the Data Collector Security Manager to prevent access to Data Collector internal directories to all stages (e.g. data dir,\r\n# ...). Please note that there are certain JVM bugs that this manager might hit, especially on some older JVM versions.\r\n#security_manager.sdc_manager.enable=true\r\n\r\n# When Security Manager is enabled Data Collector will by default prohibit access to its internal directories regardless of what\r\n# the security policy specifies. The following properties allow specific access to given files inside protected directories.\r\n# General exceptions - use with caution, all stage libraries will be able to access those files.\r\n# * Access to ldap-login.conf is sadly required by the Hadoop UserGroupInfo class\r\nsecurity_manager.sdc_dirs.exceptions=$SDC_CONF/ldap-login.conf\r\n\r\n# Exceptions for specific stage libraries\r\n# * Our documentation recommends default name for credential store inside ETC directory\r\nsecurity_manager.sdc_dirs.exceptions.lib.streamsets-datacollector-jks-credentialstore-lib=$SDC_CONF/jks-credentialStore.pkcs12\r\n\r\n\r\n# Stage-specific configurations\r\n#\r\n# The following config properties are for particular stages, please refer to their documentation for further details.\r\n#\r\n# Hadoop components\r\n# Uncomment to enforce Hadoop components in Data Collector to always impersonate current user rather then use\r\n# the impersonation configuration option. Current user is a user who either started the pipeline or run preview.\r\n#stage.conf_hadoop.always.impersonate.current.user=true\r\n# Uncomment to enforce impersonated user name to be lower cased.\r\n#stage.conf_hadoop.always.lowercase.user=true\r\n#\r\n\r\n# Impersonate current user to connect to Hive\r\nstage.conf_com.streamsets.pipeline.stage.hive.impersonate.current.user=false\r\n\r\n# JDBC components\r\n# Drivers that should always be auto-loaded even if they are not JDBC 4 compliant or fails to load (comma separated list)\r\n#stage.conf_com.streamsets.pipeline.stage.jdbc.drivers.load=mysql.jdbc.Driver\r\n\r\n# Kafka stages\r\n# Controls where Kerberos authentication keytabs entered in stage properties are stored\r\n# stage.conf_kafka.keytab.location=/tmp/sdc\r\n\r\n# Use new version of addRecordsToQueue() in Oracle CDC origin\r\nstage.conf_com.streamsets.pipeline.stage.origin.jdbc.cdc.oracle.addrecordstoqueue=true\r\n\r\n# Shell executor\r\n# Controls impersonation mode\r\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.impersonation_mode=CURRENT_USER\r\n# Relative or absolute path to shell that should be used to execute the shell script\r\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.shell=sh\r\n# Relative or absolute path to sudo command\r\n#stage.conf_com.streamsets.pipeline.stage.executor.shell.sudo=sudo\r\n\r\n# Antenna Doctor\r\n\r\n# Antenna Doctor is a rule-based engine designed to help end-user self-diagnose most common issues and suggest\r\n# potential fixes and workarounds.\r\n\r\n# Uncomment to disable Antenna Doctor completely\r\n#antennadoctor.enable=false\r\n\r\n# Uncomment to disable periodical updates of the knowledge base from internet\r\n#antennadoctor.update.enable=false\r\n\r\n#Observer related\r\n\r\n#The size of the queueName where the pipeline queues up data rule evaluation requests.\r\n#Each request is for a stream and contains sampled records for all rules that apply to that lane.\r\nobserver.queue.size=100\r\n\r\n#Sampled records which pass evaluation are cached for user to view. This determines the size of the cache and there is\r\n#once cache per data rule\r\nobserver.sampled.records.cache.size=100\r\n\r\n#The time to wait before dropping a data rule evaluation request if the observer queueName is full.\r\nobserver.queue.offer.max.wait.time.ms=1000\r\n\r\n\r\n#Maximum number of private classloaders to allow in Data Collector.\r\n#Stage that have configuration singletons (i.e. Hadoop FS & Hbase) require private classloaders\r\nmax.stage.private.classloaders=50\r\n\r\n# Pipeline runner pool\r\n# Default value is sufficient to run 22 pipelines. One pipeline requires 5 threads and pipelines share\r\n# threads using thread pool. Approximate runner thread pool size = (Number of Running Pipelines) * 2.2.\r\n# Increasing this value will not increase parallelisation of individual pipelines.\r\nrunner.thread.pool.size=50\r\n\r\n# Uncomment to disable starting all previously running pipelines upon Data Collector start up\r\n#runner.boot.pipeline.restart=false\r\n\r\n# Maximal number of runners (multithreaded pipelines)\r\n#\r\n# Maximal number of source-less pipeline instances (=runners) that are allowed for a single multi-threaded\r\n# pipeline. The default is 50.\r\npipeline.max.runners.count=50\r\n\r\n# Uncomment to specify a custom location for Package Manager repositories.\r\n# Enter a url or comma-separated list of urls.\r\n# Official Data Collector releases use the following repositories by default:\r\n# http://archives.streamsets.com/datacollector/<version>/tarball/,\r\n# http://archives.streamsets.com/datacollector/<version>/tarball/enterprise/ and\r\n# http://archives.streamsets.com/datacollector/<version>/legacy/\r\n# Data Collector source code builds (master branch) use the following repositories by default:\r\n# http://nightly.streamsets.com/datacollector/latest/tarball/,\r\n# http://nightly.streamsets.com/datacollector/latest/tarball/enterprise/ and\r\n# http://nightly.streamsets.com/datacollector/latest/legacy/ and\r\n#package.manager.repository.links=\r\n\r\n# Support bundles\r\n#\r\n# Uncomment if you need to disable the facility for automatic support bundle upload.\r\n#bundle.upload.enabled=false\r\n#\r\n# Uncomment to automatically generate and upload bundle on various errors. Enable with caution, uploading bundle\r\n# can be time consuming task (depending on size and internet speed) and pipelines can appear \"frozen\" during\r\n# the upload especially when many pipelines are failing at the same time.\r\n#bundle.upload.on_error=true\r\n\r\n# Library aliases mapping to keep backward compatibility on pipelines when library names change\r\n# The current aliasing mapping is to handle 1.0.0beta2 to 1.0.0 library names changes\r\n#\r\n# IMPORTANT: Under normal circumstances all these properties should not be changed\r\n#\r\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_1_1-lib=streamsets-datacollector-apache-kafka_0_8_1-lib\r\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_2_0-lib=streamsets-datacollector-apache-kafka_0_8_2-lib\r\nlibrary.alias.streamsets-datacollector-apache-kafka_0_8_2_1-lib=streamsets-datacollector-apache-kafka_0_8_2-lib\r\nlibrary.alias.streamsets-datacollector-cassandra_2_1_5-lib=streamsets-datacollector-cassandra_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_2_1-lib=streamsets-datacollector-cdh_5_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_2_3-lib=streamsets-datacollector-cdh_5_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_2_4-lib=streamsets-datacollector-cdh_5_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_3_0-lib=streamsets-datacollector-cdh_5_3-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_3_1-lib=streamsets-datacollector-cdh_5_3-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_3_2-lib=streamsets-datacollector-cdh_5_3-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_4_0-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_4_0-lib=streamsets-datacollector-cdh_5_4-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_4_1-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh5_4_1-lib=streamsets-datacollector-cdh_5_4-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_5_4-cluster-cdh_kafka_1_2-lib\r\nlibrary.alias.streamsets-datacollector-cdh_kafka_1_2_0-lib=streamsets-datacollector-cdh_kafka_1_2-lib\r\nlibrary.alias.streamsets-datacollector-elasticsearch_1_4_4-lib=streamsets-datacollector-elasticsearch_1_4-lib\r\nlibrary.alias.streamsets-datacollector-elasticsearch_1_5_0-lib=streamsets-datacollector-elasticsearch_1_5-lib\r\nlibrary.alias.streamsets-datacollector-hdp_2_2_0-lib=streamsets-datacollector-hdp_2_2-lib\r\nlibrary.alias.streamsets-datacollector-jython_2_7_0-lib=streamsets-datacollector-jython_2_7-lib\r\nlibrary.alias.streamsets-datacollector-mongodb_3_0_2-lib=streamsets-datacollector-mongodb_3-lib\r\nlibrary.alias.streamsets-datacollector-cassandra_2-lib=streamsets-datacollector-cassandra_3-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_9-cluster-cdh_kafka_2_0-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_10-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_11-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_12-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_13-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\nlibrary.alias.streamsets-datacollector-cdh_5_14-cluster-cdh_kafka_2_1-lib=streamsets-datacollector-cdh-spark_2_1-lib\r\n\r\n\r\n# Stage aliases for mapping to keep backward compatibility on pipelines when stages move libraries\r\n# The current alias mapping is to handle moving the jdbc stages to their own library\r\n#\r\n# IMPORTANT: Under normal circumstances all these properties should not be changed\r\n#\r\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_destination_jdbc_JdbcDTarget\r\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource=streamsets-datacollector-jdbc-lib,com_streamsets_pipeline_stage_origin_jdbc_JdbcDSource\r\nstage.alias.streamsets-datacollector-basic-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource=streamsets-datacollector-omniture-lib,com_streamsets_pipeline_stage_origin_omniture_OmnitureDSource\r\nstage.alias.streamsets-datacollector-cdh_5_7-cluster-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget=streamsets-datacollector-cdh_kafka_2_0-lib,com_streamsets_pipeline_stage_destination_kafka_KafkaDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_6-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_1_7-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_1-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_2-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_3-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_2_4-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-elasticsearch_5_0-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget=streamsets-datacollector-elasticsearch_5-lib,com_streamsets_pipeline_stage_destination_elasticsearch_ToErrorElasticSearchDTarget\r\nstage.alias.streamsets-datacollector-cdh_5_4-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-cdh_5_5-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-cdh_5_7-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-cdh_5_8-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-cdh_5_9-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-cdh_5_10-lib,com_streamsets_pipeline_stage_processor_spark_StandaloneSparkDProcessor=streamsets-datacollector-cdh_spark_2_1_r1-lib,com_streamsets_pipeline_stage_processor_spark_SparkDProcessor\r\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_FirehoseDTarget\r\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_StatsKinesisDTarget\r\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_KinesisDTarget\r\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_destination_kinesis_ToErrorKinesisDTarget\r\nstage.alias.streamsets-datacollector-aws-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource=streamsets-datacollector-kinesis-lib,com_streamsets_pipeline_stage_origin_kinesis_KinesisDSource\r\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor\r\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget\r\nstage.alias.streamsets-datacollector-hdp_2_3-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_3-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget\r\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_processor_hive_HiveMetadataDProcessor\r\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveMetastoreDTarget\r\nstage.alias.streamsets-datacollector-hdp_2_4-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget=streamsets-datacollector-hdp_2_4-hive1-lib,com_streamsets_pipeline_stage_destination_hive_HiveDTarget\r\nstage.alias.streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseConnectorDTarget=streamsets-datacollector-couchbase_5-lib,com_streamsets_pipeline_stage_destination_couchbase_CouchbaseDTarget\r\n\r\nstages.library.libs.remove.manager.links=streamsets-datacollector-aws-lib,streamsets-datacollector-kinesis-lib,streamsets-datacollector-azure-lib,streamsets-datacollector-google-cloud-lib,streamsets-datacollector-apache-pulsar_2-lib,streamsets-datacollector-salesforce-lib,streamsets-datacollector-couchbase_5-lib,streamsets-datacollector-databricks-ml_2-lib,streamsets-datacollector-mleap-lib,streamsets-datacollector-tensorflow-lib,streamsets-datacollector-bigtable-lib\r\nstages.library.stage.remove.manager.links=com.streamsets.pipeline.stage.executor.databricks.DatabricksJobLauncherDExecutor,com.streamsets.pipeline.stage.processor.controlHub.ControlHubApiDProcessor,com.streamsets.pipeline.stage.executor.emailexecutor.EmailDExecutor,com.streamsets.pipeline.stage.processor.startJob.StartJobDProcessor,com.streamsets.pipeline.stage.origin.startJob.StartJobDSource,com.streamsets.pipeline.stage.processor.waitForJobCompletion.WaitForJobCompletionDProcessor,com.streamsets.pipeline.stage.destination.sdcipc.StatsSdcIpcDTarget,com.streamsets.pipeline.stage.destination.cassandra.CassandraDTarget,com.streamsets.pipeline.stage.processor.waitForPipelineCompletion.WaitForPipelineCompletionDProcessor,com.streamsets.pipeline.stage.processor.geolocation.GeolocationDProcessor,com.streamsets.pipeline.stage.origin.sdcipcwithbuffer.SdcIpcWithDiskBufferDSource,com.streamsets.pipeline.stage.origin.ipctokafka.SdcIpcToKafkaDSource,com.streamsets.pipeline.stage.origin.jdbc.SapHanaDSource,com.streamsets.pipeline.stage.origin.httpserver.nifi.NiFiHttpServerDPushSource,com.streamsets.pipeline.stage.origin.grpcclient.GrpcClientDSource\r\n# System and user stage libraries whitelists and blacklists\r\n#\r\n# If commented out all stagelibraries directories are used.\r\n#\r\n# Given 'system' or 'user', only whitelist or blacklist can be set, if both are set the Data Collector will fail to start\r\n#\r\n# Specify stage library directories separated by commas\r\n#\r\n# The MapR stage libraries are disabled as they require manual installation step. Use the setup-mapr script to enable\r\n# the desired MapR stage library.\r\n#\r\n# It's important to keep the blacklist and whitelist properties on a single line, otherwise CSD's control.sh script and\r\n# setup-mapr script will not work properly.\r\n#\r\n#system.stagelibs.whitelist=\r\nsystem.stagelibs.blacklist=streamsets-datacollector-mapr_6_0-lib,streamsets-datacollector-mapr_6_0-mep4-lib,streamsets-datacollector-mapr_6_0-mep5-lib,streamsets-datacollector-mapr_6_1-lib,streamsets-datacollector-mapr_6_1-mep6-lib\r\n#\r\n#user.stagelibs.whitelist=\r\n#user.stagelibs.blacklist=\r\n\r\n# Stage Classpath Validation\r\n#\r\n# Uncomment to disable best effort validation of each stage library classpath to detect known issues with\r\n# colliding dependencies (such as conflicting versions of the same dependency, ...). Result of the validation\r\n# is by default only printed to log.\r\n#stagelibs.classpath.validation.enable=false\r\n#\r\n# By default the validation result is only logged. Uncomment to prevent Data Collector to start if classpath of any\r\n# stage library is not considered valid.\r\n#stagelibs.classpath.validation.terminate=true\r\n\r\n# Health Inspector Configuration\r\n#\r\n# Configuration options specific to alter behavior of health inspector.\r\n#\r\n#health_inspector.network.host=www.streamsets.com\r\n\r\n#\r\n# Additional configuration files to include in to the configuration.\r\n# Value of this property is the name of the configuration file separated by commas.\r\n#\r\nconfig.includes=dpm.properties,vault.properties,credential-stores.properties\r\n\r\n#\r\n# Record sampling configurations indicate the size of the subset (sample set) that must be chosen from a population (of records).\r\n# Default configuration values indicate the sampler to select 1 out of 10000 records\r\n#\r\n# For better performance simplify the fraction ( sdc.record.sampling.sample.size / sdc.record.sampling.population.size )\r\n# i.e., specify ( 1 / 40 ) instead of ( 250 / 10000 ).\r\nsdc.record.sampling.sample.size=1\r\nsdc.record.sampling.population.size=10000\r\n\r\n#\r\n# Pipeline state are cached for faster access.\r\n# Specifies the maximum number of pipeline state entries the cache may contain.\r\nstore.pipeline.state.cache.maximum.size=100\r\n\r\n# Specifies that each pipeline state entry should be automatically removed from the cache once a fixed duration\r\n# has elapsed after the entry's creation, the most recent replacement of its value, or its last access.\r\n# In minutes\r\nstore.pipeline.state.cache.expire.after.access=10\r\n\r\n# pipeline store module\r\n# dataStore module store in local, objectStore module store in minio\r\nweb.pipeline.store.module=dataStore\r\nweb.pipeline.minio.store.endpoint=fake-http://10.206.68.1:30290\r\nweb.pipeline.minio.store.accessKey=fake-YOURACCESSKEY\r\nweb.pipeline.minio.store.secretKey=fake-YOURSECRETKEY\r\nweb.pipeline.minio.store.bucket.name=fake-data-thread\r\n"
  support-bundle-redactor.json: "{\r\n  \"version\": 1,\r\n  \"rules\": [\r\n    {\r\n      \"description\": \"Generic password in configuration files.\",\r\n      \"trigger\": \"password\",\r\n      \"search\": \"password=[^\\\"' ]*\",\r\n      \"replace\": \"password=REDACTED\"\r\n    }, {\r\n      \"description\": \"DPM Authorization token\",\r\n      \"trigger\": \"dpm.appAuthToken\",\r\n      \"search\": \"dpm.appAuthToken=[^\\\"' ]*\",\r\n      \"replace\": \"dpm.appAuthToken=REDACTED\"\r\n    }, {\r\n      \"description\": \"LDAP Password\",\r\n      \"trigger\": \"bindPassword\",\r\n      \"search\": \"bindPassword=[^\\\"' ]*\",\r\n      \"replace\": \"bindPassword=REDACTED\"\r\n    }, {\r\n      \"description\": \"SDC Sensitive Configs\",\r\n      \"trigger\": \".password=\",\r\n      \"search\": \"^(https.cluster.keystore.password|https.keystore.password|https.truststore.password|https.cluster.truststore.password|xmail.password|lineage.publisher.navigator.config.password)=.*$\",\r\n      \"replace\": \"$1=REDACTED\"\r\n    }\r\n  ]\r\n}\r\n"
  vault.properties: "#\r\n# Copyright 2017 StreamSets Inc.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\n#\r\n# Vault EL functions, vault:read() and vault:readWithDelay(), are deprecated.\r\n# Use instead the CredentialStore EL functions with a Vault implementation.\r\n#\r\n# Vault configuration should be set in the credential-stores.properties file. Previous Vault configuration property\r\n# names must be added to the credentials-store.properties file prefixed with 'credentialStore.vault.'.\r\n#\r\n#\r\n# For backwards compatibility, you can specify with the following property, the ID of the Vault CredentialStore\r\n# you want to be used with the deprecated functions.\r\n#\r\n\r\n#vaultEL.credentialStore.id=vault\r\n"
